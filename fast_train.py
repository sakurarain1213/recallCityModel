
"""
åˆ†æ‰¹è®­ç»ƒè„šæœ¬ (Checkpoint & æé€Ÿä¼˜åŒ–ç‰ˆ)
åŠŸèƒ½: 
1. æ”¯æŒæ¯ N è½®ä¿å­˜ Checkpoint
2. ä½¿ç”¨ Mini-Validation Set åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°
3. ç§»é™¤è®­ç»ƒé›†å®æ—¶è¯„ä¼°ï¼Œå¤§å¹…æé€Ÿ
"""
import lightgbm as lgb
import pandas as pd
import numpy as np
import gc
import time
import argparse
import os
from pathlib import Path
from src.config import Config
import matplotlib.pyplot as plt
import seaborn as sns

def print_log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")

def calculate_sample_weights(df, end_year, decay_rate=0.9):
    """
    è®¡ç®—æ ·æœ¬æƒé‡ï¼šæ—¶é—´è¡°å‡ + å·®å¼‚åŒ–æ ·æœ¬æƒé‡

    å‚æ•°:
    - df: è®­ç»ƒæ•°æ® DataFrame (å¿…é¡»åŒ…å« Rank, Label, Year åˆ—)
    - end_year: ç›®æ ‡é¢„æµ‹å¹´ä»½
    - decay_rate: æ—¶é—´è¡°å‡ç‡ (é»˜è®¤ 0.9ï¼Œå³æ¯å¹´è¡°å‡ 10%)

    è¿”å›:
    - weights: æ ·æœ¬æƒé‡æ•°ç»„
    """
    # 1. æ—¶é—´è¡°å‡æƒé‡
    year_diff = end_year - df['Year']
    time_weights = decay_rate ** year_diff

    # 2. æ ·æœ¬ç±»å‹æƒé‡
    base_weights = np.ones(len(df), dtype=np.float32)

    # æ­£æ ·æœ¬ (Rank 1-10, Label=1)
    pos_mask = (df['Label'] == 1.0)
    if pos_mask.any():
        rank = df['Rank'].copy()
        # å¤´éƒ¨ä¿æŠ¤ (Rank 1-3)
        top3_mask = pos_mask & (rank <= 3)
        base_weights[top3_mask] = 20.0
        # å…¶ä»–æ­£æ ·æœ¬ (Rank 4-10)
        other_pos_mask = pos_mask & (rank > 3) & (rank <= 10)
        base_weights[other_pos_mask] = 10.0

    # å›°éš¾è´Ÿæ ·æœ¬ (Rank 11-20, Label=0) - Hard Negative Mining
    hard_neg_mask = (df['Label'] == 0.0) & (df['Rank'] > 10) & (df['Rank'] <= 20)
    base_weights[hard_neg_mask] = 5.0

    # æ™®é€šè´Ÿæ ·æœ¬ (Rank > 20 æˆ– Rank ä¸º 97/98/99) æƒé‡ä¿æŒä¸º 1.0

    # 3. ç»„åˆæƒé‡
    final_weights = base_weights * time_weights

    return final_weights

def load_data_batch(years, shuffle=True):
    """
    åŠ è½½æŒ‡å®šå¹´ä»½çš„æ•°æ®ä½œä¸ºä¸€ä¸ªBatch
    """
    dfs = []
    print_log(f"   ğŸ“¥ Loading parquet for years: {years}")
    for year in years:
        p = Path(Config.PROCESSED_DIR) / f"processed_{year}.parquet"
        if p.exists():
            df = pd.read_parquet(p)
            # ç®€å•çš„é˜²å¾¡æ€§ç±»å‹è½¬æ¢
            for c in ['From_City', 'To_City']:
                if c in df.columns: df[c] = df[c].astype('int16')
            if 'Label' in df.columns: df['Label'] = df['Label'].astype('float32')
            dfs.append(df)
    
    if not dfs: return None
    
    # åˆå¹¶
    df_batch = pd.concat(dfs, axis=0, ignore_index=True)

    # ç”Ÿæˆ qid (Query ID) å¦‚æœä¸å­˜åœ¨
    # qid ç”¨äºæŒ‰ Query å®Œæ•´é‡‡æ ·,ç¡®ä¿éªŒè¯é›†çš„ Recall æŒ‡æ ‡å‡†ç¡®
    if 'qid' not in df_batch.columns:
        print_log("   ğŸ†” Generating qid (Query ID) for batch...")
        # parquet æ–‡ä»¶ä¸­ Type_ID è¢«è½¬ä¸º Type_Hashï¼Œä½¿ç”¨å®ƒæ¥åŒºåˆ†ä¸åŒç±»å‹
        if 'Type_Hash' in df_batch.columns:
            df_batch['qid'] = (
                df_batch['Year'].astype('int64') * 100000 +
                df_batch['Type_Hash'].astype('int64') % 1000 +  # å–æ¨¡é¿å…æ•°å€¼è¿‡å¤§
                df_batch['From_City'].astype('int64')
            ).astype('int64')
        else:
            # é™çº§æ–¹æ¡ˆï¼šåªç”¨ Year + From_City
            df_batch['qid'] = (
                df_batch['Year'].astype('int64') * 100000 +
                df_batch['From_City'].astype('int64')
            ).astype('int64')

    # ç¡®ä¿ Rank åˆ—å­˜åœ¨ (ç”¨äºè®¡ç®—æƒé‡)
    if 'Rank' not in df_batch.columns:
        print_log("   âš ï¸ Warning: Rank column not found, weights will be uniform")

    # Batchå†…éƒ¨æ‰“ä¹±
    if shuffle:
        print_log(f"   ğŸ”€ Shuffling {len(df_batch):,} rows...")
        df_batch = df_batch.sample(frac=1, random_state=42).reset_index(drop=True)

    return df_batch

# === æ–°å¢ï¼šCheckpoint å›è°ƒå‡½æ•° ===
def save_checkpoint_callback(save_freq, output_dir, year_prefix):
    """
    æ¯ save_freq è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    """
    def callback(env):
        # env.iteration ä» 0 å¼€å§‹
        iteration = env.iteration + 1
        if iteration % save_freq == 0:
            # æ„é€ æ–‡ä»¶å: checkpoints/model_2010_round_50.txt
            ckpt_dir = Path(output_dir) / "checkpoints"
            ckpt_dir.mkdir(exist_ok=True)
            path = ckpt_dir / f"model_{year_prefix}_round_{iteration}.txt"
            env.model.save_model(str(path))
            print_log(f"   ğŸ’¾ Checkpoint saved: {path.name}")
    return callback

def train_batch_mode(target_end_year, batch_size_years=5, checkpoint_freq=50):
    total_start = time.time()
    print("="*60)
    print(f"ğŸš€ Batch Training Task: End Year {target_end_year}")
    print(f"ğŸ“¦ Batch Size: {batch_size_years} Years")
    print(f"â±ï¸ Checkpoint Frequency: Every {checkpoint_freq} rounds")
    print("="*60)

    # 1. è§„åˆ’ Batches
    all_train_years = list(range(2001, target_end_year - 2))
    val_years = [target_end_year - 2, target_end_year - 1]

    batches = [all_train_years[i:i + batch_size_years] for i in range(0, len(all_train_years), batch_size_years)]
    
    print_log(f"ğŸ“… Training Sequence: {batches}")
    print_log(f"ğŸ“… Validation Years: {val_years}")

    # 2. å‡†å¤‡éªŒè¯é›†
    print_log("\nğŸ“¦ Loading Validation Data (Global)...")
    df_val = load_data_batch(val_years, shuffle=False)

    # ã€ç²¾åº¦ä¼˜åŒ–ã€‘æŒ‰ Query å®Œæ•´é‡‡æ ·,ä¸éšæœºæ‹†åˆ†è¡Œ
    # æ„é€ ä¸€ä¸ªæå°çš„éªŒè¯é›† (20ä¸‡è¡Œ) ä¸“é—¨ç”¨äº Early Stopping å’Œ å®æ—¶æ‰“å°
    # å…³é”®: æŒ‰ qid åˆ†ç»„,ç¡®ä¿ä¸€ä¸ª Query çš„æ‰€æœ‰æ ·æœ¬éƒ½åœ¨éªŒè¯é›†ä¸­
    WATCH_SIZE = 200000

    if len(df_val) > WATCH_SIZE:
        print_log(f"âš¡ Creating Mini-Validation Set for Speed: ~{WATCH_SIZE:,} rows")
        print_log(f"   ğŸ“Š Sampling by complete queries (qid) to preserve Recall metric...")

        # è®¡ç®—éœ€è¦çš„ query æ•°é‡
        avg_samples_per_query = len(df_val) / df_val['qid'].nunique()
        n_queries_needed = int(WATCH_SIZE / avg_samples_per_query)

        # éšæœºé‡‡æ ·å®Œæ•´çš„ query
        unique_qids = df_val['qid'].unique()
        sampled_qids = pd.Series(unique_qids).sample(n=n_queries_needed, random_state=42).values

        # ä¿ç•™è¿™äº› query çš„æ‰€æœ‰æ ·æœ¬
        df_val_watch = df_val[df_val['qid'].isin(sampled_qids)].reset_index(drop=True)
        print_log(f"   âœ… Sampled {len(sampled_qids):,} queries -> {len(df_val_watch):,} rows")

        # é‡Šæ”¾åŸå§‹å¤§è¡¨
        del df_val
        gc.collect()
    else:
        df_val_watch = df_val
        del df_val

    # ç‰¹å¾è¯†åˆ«
    excludes = ['Year', 'From_City', 'To_City', 'Label', 'Rank', 'Flow_Count', 'qid']
    feats = [c for c in df_val_watch.columns if c not in excludes and not c.endswith('_orig')]
    cats = ['gender', 'age_group', 'education', 'industry', 'income', 'family', 'is_same_province']
    cats = [c for c in cats if c in feats]
    
    print_log(f"âœ¨ Features: {len(feats)} | Categorical: {len(cats)}")

    # æ„å»º Mini éªŒè¯é›† Dataset
    print_log("ğŸ”¨ Constructing Watch Dataset...")
    val_ds_watch = lgb.Dataset(
        df_val_watch[feats], 
        label=df_val_watch['Label'], 
        categorical_feature=cats, 
        params=Config.LGBM_PARAMS, 
        free_raw_data=False 
    )
    val_ds_watch.construct()
    
    # é‡Šæ”¾ Pandas å¯¹è±¡
    del df_val_watch
    gc.collect()

    # 3. å¾ªç¯è®­ç»ƒ (Incremental Learning)
    model = None
    
    for i, batch_years in enumerate(batches):
        print("\n" + "-"*40)
        print_log(f"ğŸ”„ Processing Batch {i+1}/{len(batches)}: Years {batch_years}")
        print("-"*40)
        
        # åŠ è½½ -> æ‰“ä¹±
        df_train = load_data_batch(batch_years, shuffle=True)
        if df_train is None: continue

        # ======================= ã€ç‰¹å¾å¯¹é½ã€‘ =======================
        # ğŸ›¡ï¸ ç¡®ä¿è®­ç»ƒé›†åŒ…å«æ‰€æœ‰éªŒè¯é›†æœ‰çš„ç‰¹å¾
        # å¦‚æœæ—©æœŸå¹´ä»½ç¼ºå°‘ 5y_avg ç­‰ç‰¹å¾ï¼Œæ‰‹åŠ¨è¡¥ä¸Šå¹¶å¡« -1
        missing_cols = [c for c in feats if c not in df_train.columns]
        if missing_cols:
            print_log(f"   âš ï¸ Aligning features: Filling {len(missing_cols)} missing cols (e.g., {missing_cols[0]}) with -1")
            for c in missing_cols:
                df_train[c] = -1.0
                df_train[c] = df_train[c].astype('float32')
        # ======================= ã€ç‰¹å¾å¯¹é½ç»“æŸã€‘ =======================

        print_log(f"   Rows: {len(df_train):,} | Memory: {df_train.memory_usage(deep=True).sum()/1024**3:.2f} GB")

        # ======================= ã€æ ·æœ¬æƒé‡è®¡ç®—ã€‘ =======================
        # è®¡ç®—æ ·æœ¬æƒé‡ï¼šæ—¶é—´è¡°å‡ + å·®å¼‚åŒ–æƒé‡
        print_log("   ğŸ¯ Calculating sample weights (Time Decay + Reweighting)...")
        weights = calculate_sample_weights(df_train, target_end_year, decay_rate=0.9)

        # æ‰“å°æƒé‡ç»Ÿè®¡
        print_log(f"   ğŸ“Š Weight stats: min={weights.min():.4f}, max={weights.max():.4f}, mean={weights.mean():.4f}")
        # ======================= ã€æƒé‡è®¡ç®—ç»“æŸã€‘ =======================

        # æ„å»º Dataset
        t_build = time.time()
        train_ds = lgb.Dataset(
            df_train[feats],
            label=df_train['Label'],
            weight=weights,  # åº”ç”¨æ ·æœ¬æƒé‡
            categorical_feature=cats,
            params=Config.LGBM_PARAMS,
            free_raw_data=False
        )
        train_ds.construct()
        print_log(f"   Dataset Built: {time.time()-t_build:.1f}s")
        
        del df_train
        gc.collect()
        
        # è®­ç»ƒ
        print_log("   ğŸ”¥ Training...")
        try:
            # å›è°ƒåˆ—è¡¨
            callbacks_list = [
                # æ ¸å¿ƒæ—©åœå‚æ•° ä¿®æ”¹åï¼šå¢åŠ è€å¿ƒåˆ° 50 æˆ– 100ï¼Œæˆ–è€…ç›´æ¥æ³¨é‡Šæ‰ï¼Œè®©å®ƒè·‘æ»¡ 1000 è½®
                lgb.early_stopping(stopping_rounds=100, verbose=True),
                lgb.log_evaluation(50), # å‡å°‘æ‰“å°é¢‘ç‡åˆ° 50
                # æ·»åŠ  Checkpoint å›è°ƒ
                save_checkpoint_callback(checkpoint_freq, Config.OUTPUT_DIR, target_end_year)
            ]

            model = lgb.train(
                Config.LGBM_PARAMS,
                train_ds,
                num_boost_round=1000, 
                # ã€æé€Ÿå…³é”®ã€‘valid_sets åªæ”¾ mini éªŒè¯é›†ï¼Œä¸”ä¸æ”¾è®­ç»ƒé›†
                valid_sets=[val_ds_watch], 
                valid_names=['val_mini'], # æ”¹ä¸ªåå­—åŒºåˆ†
                init_model=model,            
                keep_training_booster=True,  
                callbacks=callbacks_list
            )
        except Exception as e:
            print_log(f"âŒ Training failed at batch {i+1}: {e}")
            raise e
        finally:
            del train_ds
            gc.collect()

    # 4. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    out_path = Path(Config.OUTPUT_DIR) / f'lgb_batch_end_{target_end_year}.txt'
    if model:
        model.save_model(str(out_path))
        print_log(f"\nâœ… All Batches Finished! Total time: {(time.time() - total_start)/60:.1f} min")
        print_log(f"ğŸ’¾ Model saved to: {out_path}")

        # ç»˜å›¾
        print("\n" + "="*40)
        print("ğŸ“Š Feature Importance (Gain)")
        print("="*40)
        
        importance = model.feature_importance(importance_type='gain')
        feature_names = model.feature_name()
        
        fi_df = pd.DataFrame({'feature': feature_names, 'importance': importance})
        fi_df = fi_df.sort_values(by='importance', ascending=False)
        
        print(fi_df.head(20).to_string(index=False))
        
        plt.figure(figsize=(12, 10))
        sns.barplot(x='importance', y='feature', data=fi_df.head(30))
        plt.title(f'LightGBM Feature Importance (Gain) - End Year {target_end_year}')
        plt.tight_layout()
        plt.savefig(Path(Config.OUTPUT_DIR) / f'feature_importance_{target_end_year}.png')
        print(f"\nğŸ–¼ï¸ Feature importance plot saved to output/feature_importance_{target_end_year}.png")
        
        geo_rank = fi_df[fi_df['feature'] == 'geo_distance'].index
        if len(geo_rank) > 0 and geo_rank[0] > 5:
            print("\nâš ï¸ è­¦å‘Š: åœ°ç†è·ç¦» (geo_distance) æƒé‡è¿‡ä½ï¼è¯·æ£€æŸ¥ city_edges æ˜¯å¦æ­£ç¡® Mergeï¼")

        print("="*40)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--end_year', type=int, default=2020, help='Target End Year')
    parser.add_argument('--batch_size', type=int, default=5, help='Years per batch')
    parser.add_argument('--ckpt_freq', type=int, default=50, help='Checkpoint frequency')
    args = parser.parse_args()
    
    train_batch_mode(args.end_year, args.batch_size, args.ckpt_freq)