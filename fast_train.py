"""
æé€Ÿè®­ç»ƒè„šæœ¬ (è°ƒè¯•å¢å¼ºç‰ˆ)
åŠŸèƒ½: åŒ…å«è¯¦ç»†è€—æ—¶ç›‘æ§ã€å†…å­˜ç›‘æ§ã€æ˜¾å¼ Dataset æ„å»º
"""
import lightgbm as lgb
import pandas as pd
import gc
import time
import argparse
import sys
from pathlib import Path
from src.config import Config

# æé€Ÿé…ç½®
FAST_PARAMS = {
    'objective': 'binary',
    'metric': ['binary_logloss', 'auc'],
    'boosting_type': 'goss',      # æ ¸å¿ƒæé€Ÿ
    'top_rate': 0.2,
    'other_rate': 0.1,
    'num_leaves': 63,
    'max_depth': 8,
    'max_bin': 63,                # æ ¸å¿ƒæé€Ÿ
    'learning_rate': 0.1,
    'n_estimators': 1000,
    'colsample_bytree': 0.8,
    'min_child_samples': 100,
    'lambda_l1': 0.1,
    'lambda_l2': 0.1,
    'n_jobs': 24,
    'verbosity': -1
}

def print_log(msg):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—"""
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")

def load_data_silent(years, data_dir):
    dfs = []
    total_rows = 0
    t0 = time.time()
    
    # é¢„åˆ†é…åˆ—è¡¨ä»¥å‡å°‘å†…å­˜ç¢ç‰‡
    print_log(f"æ­£åœ¨åŠ è½½ {len(years)} ä¸ªå¹´ä»½çš„æ•°æ®...")
    
    for year in years:
        p = Path(data_dir) / f"train_{year}.parquet"
        if p.exists():
            # åªè¯»å–éœ€è¦çš„åˆ—? (æš‚ä¸ä¼˜åŒ–ï¼Œå‡è®¾å·²æœ‰å†…å­˜è¶³å¤Ÿ)
            df = pd.read_parquet(p)
            
            # é˜²å¾¡æ€§è½¬æ¢ (ç¡®ä¿ float32/int16)
            for c in ['From_City', 'To_City']:
                if c in df.columns and df[c].dtype != 'int16': 
                    df[c] = df[c].astype('int16')
            if 'Label' in df.columns and df['Label'].dtype != 'float32': 
                df['Label'] = df['Label'].astype('float32')
            
            dfs.append(df)
            total_rows += len(df)
            # print(f"  -> Loaded {year} ({len(df):,} rows)")
    
    if not dfs: return None
    
    print_log(f"åˆå¹¶ {len(dfs)} ä¸ª DataFrame...")
    res = pd.concat(dfs, axis=0, ignore_index=True)
    print_log(f"åŠ è½½å®Œæˆ: {total_rows:,} è¡Œ, è€—æ—¶ {time.time()-t0:.1f}s")
    return res

def train_fast(target_end_year):
    total_start = time.time()
    print("="*60)
    print(f"ğŸš€ Training Task: End Year {target_end_year}")
    print("="*60)
    
    # 1. åˆ’åˆ†æ•°æ®é›†
    train_years = list(range(2001, target_end_year - 2))
    val_years = [target_end_year - 2, target_end_year - 1]
    
    # 2. åŠ è½½æ•°æ®
    print_log("ğŸ“¦ Loading Training Data...")
    df_train = load_data_silent(train_years, Config.PROCESSED_DIR)
    
    # å†…å­˜ç›‘æ§
    mem_usage = df_train.memory_usage(deep=True).sum() / 1024**3
    print_log(f"ğŸ“Š Training Data Memory: {mem_usage:.2f} GB")
    
    print_log("ğŸ“¦ Loading Validation Data...")
    df_val = load_data_silent(val_years, Config.PROCESSED_DIR)
    
    # 3. éªŒè¯é›†ç˜¦èº«
    if len(df_val) > 2000000:
        print_log(f"âš¡ Sampling Val: {len(df_val):,} -> 2,000,000")
        df_val = df_val.sample(n=2000000, random_state=42).reset_index(drop=True)

    # 4. å‡†å¤‡ç‰¹å¾
    excludes = ['Year', 'From_City', 'To_City', 'Label', 'Rank', 'Flow_Count', 'qid']
    feats = [c for c in df_train.columns if c not in excludes and not c.endswith('_orig')]
    cats = ['gender', 'age_group', 'education', 'industry', 'income', 'family', 'is_same_province']
    cats = [c for c in cats if c in feats]
    
    print_log(f"âœ¨ Features: {len(feats)} (Cats: {len(cats)})")

    # 5. æ„å»º Dataset (æ˜¾å¼ Construct)
    print_log("ğŸ”¨ Init Train Dataset...")
    t_ds = time.time()
    
    # free_raw_data=False: 2äº¿è¡Œæ•°æ®å»ºè®®ä¿ç•™åœ¨å†…å­˜ä¸­(å¦‚æœå¤Ÿå¤§)ï¼Œå¦åˆ™æ¯æ¬¡è¿­ä»£é‡æ–°è¯»å–ä¼šæœ‰å¼€é”€
    # ä½†å¦‚æœå†…å­˜ä¸å¤Ÿ(>100GBå ç”¨)ï¼Œè¿™é‡Œä¼šOOMï¼Œæ­¤æ—¶éœ€æ”¹ä¸º True
    train_ds = lgb.Dataset(
        df_train[feats], 
        label=df_train['Label'], 
        categorical_feature=cats, 
        params=FAST_PARAMS, 
        free_raw_data=False 
    )
    
    print_log("ğŸ”¨ Constructing Train Binning (è¿™å°†èŠ±è´¹ä¸€äº›æ—¶é—´)...")
    # æ˜¾å¼è°ƒç”¨ construct() ä»¥ä¾¿æˆ‘ä»¬çŸ¥é“è¿™ä¸€æ­¥èŠ±äº†å¤šä¹…
    train_ds.construct()
    print_log(f"âœ… Train DS Constructed. Time: {time.time()-t_ds:.1f}s")
    
    # éªŒè¯é›†
    print_log("ğŸ”¨ Init Val Dataset...")
    val_ds = lgb.Dataset(
        df_val[feats], 
        label=df_val['Label'], 
        categorical_feature=cats, 
        reference=train_ds, 
        params=FAST_PARAMS,
        free_raw_data=False
    )
    val_ds.construct() # æ˜¾å¼æ„å»º
    
    # é‡Šæ”¾ Pandas å†…å­˜ (Dataset å¦‚æœè®¾ç½®äº† free_raw_data=Falseï¼Œå®ƒä¼šæ‹·è´/å¼•ç”¨æ•°æ®ï¼Œè¿™é‡Œé‡Šæ”¾ df_train å®‰å…¨å—ï¼Ÿ)
    # å¦‚æœ free_raw_data=Falseï¼ŒLightGBM ä¼šæŒæœ‰æ•°æ®å¼•ç”¨æˆ–å‰¯æœ¬ã€‚
    # ä¸ºäº†ä¿é™©ï¼Œå…ˆåˆ é™¤ df_train çœ‹çœ‹å†…å­˜å˜åŒ–ã€‚
    del df_train, df_val
    gc.collect()
    print_log("ğŸ—‘ï¸  Pandas DataFrames deleted.")

    # 6. è®­ç»ƒ
    print_log("ğŸ”¥ Start Training Loop...")
    
    def log_callback(env):
        # å¼ºåˆ¶æ¯10è½®æ‰“å°æ—¶é—´ï¼Œç›‘æµ‹æ˜¯å¦å¡é¡¿
        if env.iteration % 10 == 0:
            elapsed = time.time() - total_start
            print(f"   [Iter {env.iteration}] {elapsed:.1f}s elapsed")

    model = lgb.train(
        FAST_PARAMS,
        train_ds,
        num_boost_round=FAST_PARAMS['n_estimators'],
        valid_sets=[train_ds, val_ds],
        valid_names=['train', 'val'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=True),
            lgb.log_evaluation(10),
            log_callback 
        ]
    )

    # 7. ä¿å­˜
    out_path = Path(Config.OUTPUT_DIR) / f'lgb_fast_end_{target_end_year}.txt'
    model.save_model(str(out_path))
    print_log(f"âœ… Finished! Total: {(time.time() - total_start)/60:.1f} min. Saved: {out_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--end_year', type=int, default=2016, help='Target End Year')
    args = parser.parse_args()
    
    train_fast(args.end_year)