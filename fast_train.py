"""
åˆ†æ‰¹è®­ç»ƒè„šæœ¬ (æœ€ç»ˆä¿®å¤ç‰ˆ)
ä¿®å¤äº† init_model å¯¼è‡´çš„ "Cannot set predictor after freed raw data" é”™è¯¯
"""
import lightgbm as lgb
import pandas as pd
import gc
import time
import argparse
from pathlib import Path
from src.config import Config

# æé€Ÿé…ç½®
FAST_PARAMS = {
    'objective': 'binary',
    'metric': ['binary_logloss', 'auc'],
    'boosting_type': 'goss',
    'top_rate': 0.2,
    'other_rate': 0.1,
    'num_leaves': 31,
    'max_depth': 8,
    'max_bin': 63,
    'learning_rate': 0.15,
    'n_estimators': 1000,
    'colsample_bytree': 0.8,
    'min_child_samples': 100,
    'lambda_l1': 0.1,
    'lambda_l2': 0.1,
    'n_jobs': 24,
    'verbosity': -1
}

def print_log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")

def load_data_batch(years, shuffle=True):
    """
    åŠ è½½æŒ‡å®šå¹´ä»½çš„æ•°æ®ä½œä¸ºä¸€ä¸ªBatch
    """
    dfs = []
    print_log(f"   ğŸ“¥ Loading parquet for years: {years}")
    for year in years:
        p = Path(Config.PROCESSED_DIR) / f"train_{year}.parquet"
        if p.exists():
            df = pd.read_parquet(p)
            # ç®€å•çš„é˜²å¾¡æ€§ç±»å‹è½¬æ¢
            for c in ['From_City', 'To_City']:
                if c in df.columns: df[c] = df[c].astype('int16')
            if 'Label' in df.columns: df['Label'] = df['Label'].astype('float32')
            dfs.append(df)
    
    if not dfs: return None
    
    # åˆå¹¶
    df_batch = pd.concat(dfs, axis=0, ignore_index=True)
    
    # Batchå†…éƒ¨æ‰“ä¹±
    if shuffle:
        print_log(f"   ğŸ”€ Shuffling {len(df_batch):,} rows...")
        df_batch = df_batch.sample(frac=1, random_state=42).reset_index(drop=True)
        
    return df_batch

def train_batch_mode(target_end_year, batch_size_years=3):
    total_start = time.time()
    print("="*60)
    print(f"ğŸš€ Batch Training Task: End Year {target_end_year}")
    print(f"ğŸ“¦ Batch Size: {batch_size_years} Years (Sequential Order)")
    print("="*60)

    # 1. è§„åˆ’ Batches
    all_train_years = list(range(2001, target_end_year - 2))
    val_years = [target_end_year - 2, target_end_year - 1]

    batches = [all_train_years[i:i + batch_size_years] for i in range(0, len(all_train_years), batch_size_years)]
    
    print_log(f"ğŸ“… Training Sequence: {batches}")
    print_log(f"ğŸ“… Validation Years: {val_years}")

    # 2. å‡†å¤‡éªŒè¯é›† (å›ºå®š)
    print_log("\nğŸ“¦ Loading Validation Data (Global)...")
    df_val = load_data_batch(val_years, shuffle=False)
    
    if len(df_val) > 2000000:
        print_log(f"âš¡ Sampling Val: {len(df_val):,} -> 2,000,000")
        df_val = df_val.sample(n=2000000, random_state=42).reset_index(drop=True)

    # ç‰¹å¾è¯†åˆ«
    excludes = ['Year', 'From_City', 'To_City', 'Label', 'Rank', 'Flow_Count', 'qid']
    feats = [c for c in df_val.columns if c not in excludes and not c.endswith('_orig')]
    cats = ['gender', 'age_group', 'education', 'industry', 'income', 'family', 'is_same_province']
    cats = [c for c in cats if c in feats]
    
    print_log(f"âœ¨ Features: {len(feats)} | Categorical: {len(cats)}")

    # é¢„æ„å»ºéªŒè¯é›† Dataset
    print_log("ğŸ”¨ Constructing Validation Dataset...")
    val_ds = lgb.Dataset(
        df_val[feats], 
        label=df_val['Label'], 
        categorical_feature=cats, 
        params=FAST_PARAMS, 
        free_raw_data=False 
    )
    val_ds.construct()
    del df_val
    gc.collect()

    # 3. å¾ªç¯è®­ç»ƒ (Incremental Learning)
    model = None
    
    for i, batch_years in enumerate(batches):
        print("\n" + "-"*40)
        print_log(f"ğŸ”„ Processing Batch {i+1}/{len(batches)}: Years {batch_years}")
        print("-"*40)
        
        # åŠ è½½ -> æ‰“ä¹±
        df_train = load_data_batch(batch_years, shuffle=True)
        if df_train is None: continue
            
        print_log(f"   Rows: {len(df_train):,} | Memory: {df_train.memory_usage(deep=True).sum()/1024**3:.2f} GB")
        
        # æ„å»º Dataset
        t_build = time.time()
        
        # ã€æ ¸å¿ƒä¿®å¤ç‚¹ã€‘: è®¾ç½® free_raw_data=False
        # LightGBM å¢é‡è®­ç»ƒéœ€è¦åŸå§‹æ•°æ®æ¥é‡æ–°è®¡ç®—æ®‹å·®
        train_ds = lgb.Dataset(
            df_train[feats], 
            label=df_train['Label'], 
            categorical_feature=cats, 
            params=FAST_PARAMS,
            free_raw_data=False  # <--- å¿…é¡»ä¸º False
        )
        train_ds.construct()
        print_log(f"   Dataset Built: {time.time()-t_build:.1f}s")
        
        # è™½ç„¶ train_ds æŒæœ‰æ•°æ®å¼•ç”¨ï¼Œä½† df_train å˜é‡æœ¬èº«å¯ä»¥åˆ äº†ä»¥å‡å°‘å¼•ç”¨è®¡æ•°
        del df_train
        gc.collect()
        
        # è®­ç»ƒ
        print_log("   ğŸ”¥ Training...")
        try:
            model = lgb.train(
                FAST_PARAMS,
                train_ds,
                num_boost_round=1000, 
                valid_sets=[train_ds, val_ds],
                valid_names=['train', 'val'],
                init_model=model,            # ç»§æ‰¿ä¸Šä¸€è½®çš„æ¨¡å‹
                keep_training_booster=True,  # å…è®¸ä¸‹ä¸€è½®ç»§ç»­è®­ç»ƒ
                callbacks=[
                    lgb.early_stopping(stopping_rounds=50, verbose=True),
                    lgb.log_evaluation(50)
                ]
            )
        except Exception as e:
            print_log(f"âŒ Training failed at batch {i+1}: {e}")
            raise e
        finally:
            # ã€å†…å­˜é‡Šæ”¾ã€‘è®­ç»ƒå®Œä¸€ä¸ªBatchåï¼Œæ‰‹åŠ¨é‡Šæ”¾ Dataset
            del train_ds
            gc.collect()

    # 4. ä¿å­˜
    out_path = Path(Config.OUTPUT_DIR) / f'lgb_batch_end_{target_end_year}.txt'
    if model:
        model.save_model(str(out_path))
        print_log(f"\nâœ… All Batches Finished! Total time: {(time.time() - total_start)/60:.1f} min")
        print_log(f"ğŸ’¾ Model saved to: {out_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--end_year', type=int, default=2020, help='Target End Year')
    parser.add_argument('--batch_size', type=int, default=3, help='Years per batch')
    args = parser.parse_args()
    
    train_batch_mode(args.end_year, args.batch_size)