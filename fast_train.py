"""
åˆ†æ‰¹è®­ç»ƒæ¨¡å¼ + å†…å­˜æè‡´ä¼˜åŒ–ç‰ˆ (Numpy-First Strategy)
"""
import gc
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
import lightgbm as lgb
import matplotlib.pyplot as plt
import pyarrow as pa
import pyarrow.parquet as pq

from src.config import Config
from src.city_data import CityDataLoader
from src.data_loader_v2 import load_raw_data_fast
from src.feature_pipeline import FeaturePipeline
from evaluate import evaluate_year, EvalContext

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def get_year_data(year, pipeline, hard_candidates, mode='train'):
    """
    è·å–ä¸€å¹´çš„æ•°æ®
    """
    cache_file = Path(Config.OUTPUT_DIR) / 'cache' / f"processed_{year}.parquet"

    # 1. å°è¯•è¯»å–ç¼“å­˜
    if cache_file.exists():
        try:
            return pd.read_parquet(cache_file, engine='pyarrow')
        except:
            pass

    print(f"  [Processing] Generating data for Year {year}...")

    # 2. åŠ è½½åŸå§‹æ•°æ®
    df = load_raw_data_fast(Config.DB_PATH, year, hard_candidates, Config.NEG_SAMPLE_RATE)
    if df.empty:
        return None

    # 3. ç‰¹å¾å·¥ç¨‹
    df = pipeline.transform(df, year, mode=mode, verbose=False)

    # 4. å†™å…¥ç¼“å­˜ (ä¼˜åŒ–ç±»å‹)
    for col in df.select_dtypes(include=['object', 'string']).columns:
        df[col] = df[col].astype('category')
    
    # å¼ºåˆ¶ float32
    f_cols = df.select_dtypes(include=['float64']).columns
    if len(f_cols) > 0:
        df[f_cols] = df[f_cols].astype('float32')

    table = pa.Table.from_pandas(df, nthreads=4)
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    pq.write_table(table, str(cache_file), compression='snappy', use_dictionary=True, write_statistics=False)

    return df

def generate_batches(end_year, start_year=2001):
    val_years = [end_year - 2, end_year - 1]
    train_end = end_year - 3

    # ã€å†…å­˜å…³é”®ã€‘æ¯æ‰¹è®­ç»ƒå¤šå°‘å¹´çš„æ•°æ® å¦‚æœå†…å­˜ä¾ç„¶ç´§å¼ ï¼Œå°†æ­¤å¤„çš„ 3 æ”¹ä¸º 2
    BATCH_SIZE = 5
    
    batches = []
    current = start_year
    batch_idx = 1

    while current <= train_end:
        batch_years = []
        for _ in range(BATCH_SIZE):
            if current <= train_end:
                batch_years.append(current)
                current += 1

        if batch_years:
            batches.append({
                'name': f'batch_{batch_idx}_{min(batch_years)}-{max(batch_years)}',
                'train_years': batch_years,
                'val_years': val_years
            })
            batch_idx += 1

    return batches, val_years

def train_dynamic(target_end_year, use_gpu=False):
    print(f"ğŸš€ å¯åŠ¨åŠ¨æ€åˆ†æ‰¹è®­ç»ƒ (Numpyä¼˜åŒ–ç‰ˆ) | ç›®æ ‡é¢„æµ‹å¹´ä»½: {target_end_year}")

    # 1. åˆå§‹åŒ–èµ„æº
    loader = CityDataLoader(Config.DATA_DIR).load_all()
    pipeline = FeaturePipeline(loader, data_dir=Path(Config.OUTPUT_DIR)/'cache')
    hard_candidates = loader.get_city_ids()

    # 2. ç”Ÿæˆ Batches
    batches, val_years = generate_batches(target_end_year, start_year=Config.DATA_START_YEAR + 1)
    
    print(f"ğŸ“… éªŒè¯é›†: {val_years}")
    for b in batches:
        print(f"  - {b['name']}: {b['train_years']}")

    # 3. é¢„åŠ è½½éªŒè¯é›† (å¸¦é‡‡æ ·ä¼˜åŒ–)
    print(f"\nğŸ“¦ é¢„åŠ è½½éªŒè¯é›† {val_years}...")
    val_dfs = []
    for yr in val_years:
        df = get_year_data(yr, pipeline, hard_candidates, mode='eval')
        if df is not None: 
            val_dfs.append(df)
            
    if not val_dfs:
        print("âŒ éªŒè¯é›†ä¸ºç©º")
        return

    full_val = pd.concat(val_dfs, axis=0, ignore_index=True)
    feature_cols = pipeline.get_feature_columns(full_val)
    
    # ã€å†…å­˜ä¼˜åŒ– Aã€‘éªŒè¯é›†é‡‡æ ·
    # 3000ä¸‡éªŒè¯é›†å¤ªå¤§ï¼Œé™åˆ¶åœ¨ 500 ä¸‡è¡Œä»¥å†…è¶³ä»¥è¯„ä¼°
    MAX_VAL_SIZE = 5000000 
    if len(full_val) > MAX_VAL_SIZE:
        print(f"  âš ï¸ éªŒè¯é›†è¿‡å¤§ ({len(full_val):,})ï¼Œé‡‡æ ·è‡³ {MAX_VAL_SIZE:,} è¡Œä»¥èŠ‚çœå†…å­˜...")
        full_val = full_val.sample(n=MAX_VAL_SIZE, random_state=42)
    
    print(f"  âš¡ è½¬æ¢éªŒè¯é›†ä¸º Numpy Float32...")
    # æ˜¾å¼è½¬æ¢ä¸º numpy float32ï¼Œé¿å…éšå¼ float64
    val_X = full_val[feature_cols].values.astype(np.float32)
    val_y = full_val['Label'].values.astype(np.float32)
    
    # ç«‹å³é‡Šæ”¾ DataFrame
    del full_val, val_dfs
    gc.collect()

    print(f"âœ… éªŒè¯é›†å°±ç»ª: {len(val_X):,} è¡Œ")

    # ã€ä¿®å¤ Batch 2 æŠ¥é”™çš„å…³é”®ã€‘
    # éªŒè¯é›†å¿…é¡»ä¿ç•™ Raw Data (False)ï¼Œå› ä¸ºå®ƒè¦è¢«å¤šä¸ª Batch é‡å¤ä½¿ç”¨
    # è®­ç»ƒé›†ä½¿ç”¨ True (èŠ‚çœå†…å­˜)ï¼ŒéªŒè¯é›†ä½¿ç”¨ False (å…¼å®¹å¤šè½®è®­ç»ƒ)

    # ã€ä¿®æ­£ã€‘å®šä¹‰ç±»åˆ«ç‰¹å¾åˆ—è¡¨
    categorical_feats = ['From_City', 'is_same_province']
    categorical_feats = [c for c in categorical_feats if c in feature_cols]

    val_ds = lgb.Dataset(
        val_X,
        label=val_y,
        feature_name=feature_cols,        # å…³é”®ï¼šä¼ å…¥ç‰¹å¾ååˆ—è¡¨
        categorical_feature=categorical_feats, # å…³é”®ï¼šæŒ‡å®šç±»åˆ«ç‰¹å¾
        free_raw_data=False
    )

    # æ³¨æ„ï¼šå› ä¸º free_raw_data=Falseï¼Œval_ds ä¼šæŒæœ‰ val_X çš„å¼•ç”¨
    # æ‰€ä»¥è¿™é‡Œä¸èƒ½åˆ é™¤ val_Xï¼Œå¦åˆ™ val_ds ä¹Ÿä¼šå¤±æ•ˆ
    # LightGBM ä¼šè‡ªåŠ¨ç®¡ç†è¿™éƒ¨åˆ†å†…å­˜ï¼ˆçº¦ 0.7GBï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…ï¼‰

    # 4. é€ Batch è®­ç»ƒ
    model = None
    model_save_path = Path(Config.OUTPUT_DIR) / 'models' / f'lgb_end_{target_end_year}.txt'
    model_save_path.parent.mkdir(parents=True, exist_ok=True)
    params = Config.LGBM_PARAMS_GPU if use_gpu else Config.LGBM_PARAMS
    evals_result = {}

    for i, batch in enumerate(batches):
        print(f"\n{'='*60}")
        print(f"ğŸƒ Training {batch['name']} ({i+1}/{len(batches)})")

        # ã€å†…å­˜ä¼˜åŒ– Bã€‘é€å¹´åŠ è½½å¹¶è½¬ Numpyï¼Œä¸è¿›è¡Œ Pandas Concat
        train_arrays = []
        train_labels = []
        total_rows = 0

        for yr in batch['train_years']:
            print(f"  ğŸ“– Loading Year {yr}...")
            df = get_year_data(yr, pipeline, hard_candidates, mode='train')
            if df is None or df.empty: continue
            
            # è¡¥é½åˆ—
            for col in feature_cols:
                if col not in df.columns: df[col] = 0
            
            # ç«‹å³è½¬ä¸º float32 numpy array
            # è¿™æ­¥æ˜¯å…³é”®ï¼šé˜²æ­¢ int å’Œ float æ··åˆå¯¼è‡´ concat åå˜æˆ float64
            arr = df[feature_cols].values.astype(np.float32)
            lbl = df['Label'].values.astype(np.float32)
            
            train_arrays.append(arr)
            train_labels.append(lbl)
            total_rows += len(arr)
            
            # ç«‹å³é‡Šæ”¾ DataFrame
            del df
            gc.collect()

        if total_rows == 0:
            print("  âš ï¸ è·³è¿‡ç©ºBatch")
            continue

        print(f"  âš¡ Merging into single Float32 matrix ({total_rows:,} rows)...")
        # ä½¿ç”¨ numpy vstack (æ¯” pandas concat çœå†…å­˜ä¸”ç±»å‹å¯æ§)
        X_train = np.vstack(train_arrays)
        y_train = np.concatenate(train_labels)

        # é‡Šæ”¾ä¸´æ—¶åˆ—è¡¨
        del train_arrays, train_labels
        gc.collect()

        # ã€ä¿®æ­£ã€‘å®šä¹‰ç±»åˆ«ç‰¹å¾åˆ—è¡¨ (LightGBM éœ€è¦çŸ¥é“å“ªäº›åˆ—æ˜¯ç±»åˆ«)
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯ç‰¹å¾åï¼Œå¿…é¡»ç¡®ä¿è¿™äº›åˆ—åœ¨ feature_cols ä¸­
        categorical_feats = ['From_City', 'is_same_province']
        # ç¡®ä¿åªåŒ…å«å­˜åœ¨çš„åˆ—
        categorical_feats = [c for c in categorical_feats if c in feature_cols]

        print(f"  ğŸ“¦ Constructing LGBM Dataset (Categorical: {categorical_feats})...")
        # ã€ä¿®æ­£ã€‘æ˜¾å¼ä¼ å…¥ feature_name å’Œ categorical_feature
        train_ds = lgb.Dataset(
            X_train,
            label=y_train,
            feature_name=feature_cols,        # å…³é”®ï¼šä¼ å…¥ç‰¹å¾ååˆ—è¡¨
            categorical_feature=categorical_feats, # å…³é”®ï¼šæŒ‡å®šç±»åˆ«ç‰¹å¾
            free_raw_data=True
        )
        
        # ç«‹å³é‡Šæ”¾å·¨å¤§çš„ Numpy æ•°ç»„
        del X_train, y_train
        gc.collect()

        # è®­ç»ƒ
        print(f"  ğŸ”¥ Fitting model...")
        model = lgb.train(
            params,
            train_ds,
            num_boost_round=params['n_estimators'],
            valid_sets=[train_ds, val_ds],
            valid_names=['train', 'val'],
            init_model=model,
            callbacks=[
                lgb.log_evaluation(10),
                lgb.early_stopping(50),
                lgb.record_evaluation(evals_result)
            ]
        )
        
        del train_ds
        gc.collect()

    if model:
        model.save_model(str(model_save_path))
        print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_save_path}")

        # ã€æ–°å¢ã€‘æ‰“å°å¹¶ä¿å­˜ç‰¹å¾é‡è¦æ€§åˆ—è¡¨
        print_and_plot_importance(model, target_end_year)
        plot_history(evals_result, target_end_year)

        # ã€å¿«é€Ÿè¯„ä¼°ã€‘åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ åœ¨æµ‹è¯•é›† {Config.TEST_YEARS} ä¸Šå¿«é€Ÿè¯„ä¼°...")

        # åˆå§‹åŒ–è¯„ä¼°ä¸Šä¸‹æ–‡
        ctx = EvalContext()
        # åŠ è½½åˆšæ‰è®­ç»ƒå¥½çš„æ¨¡å‹
        ctx.load_resources(model_save_path)

        # è¯„ä¼°é…ç½®ä¸­å®šä¹‰çš„æµ‹è¯•å¹´ä»½ (é€šå¸¸æ˜¯ target_end_year - 1 æˆ– target_end_year)
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è¯„ä¼° target_end_year è¿™ä¸€å¹´
        # æ³¨æ„ï¼šcache_dir æŒ‡å‘è®­ç»ƒç”Ÿæˆçš„æ•°æ®ç›®å½•
        CACHE_DIR = Path(Config.OUTPUT_DIR) / 'cache'

        # ä½¿ç”¨ evaluate_year (è¿™æ˜¯ evaluate.py ä¸­çš„ä¸»å‡½æ•°)
        evaluate_year(target_end_year, ctx, sample_size=50000, cache_dir=CACHE_DIR)

def print_and_plot_importance(model, year):
    """
    ã€æ–°å¢ã€‘æ‰“å°æ–‡æœ¬ç‰ˆç‰¹å¾é‡è¦æ€§å¹¶ä¿å­˜å›¾è¡¨
    """
    # 1. è·å–ç‰¹å¾é‡è¦æ€§
    importance = model.feature_importance(importance_type='gain')
    names = model.feature_name()

    # 2. æ„å»º DataFrame
    df_imp = pd.DataFrame({'feature': names, 'gain': importance})
    df_imp = df_imp.sort_values(by='gain', ascending=False).reset_index(drop=True)

    # 3. æ‰“å° Top 20 åˆ°æ§åˆ¶å°
    print(f"\nğŸ“Š Feature Importance (Top 20) - End {year}")
    print("-" * 60)
    print(f"{'Rank':<5} {'Feature':<30} {'Gain':<15} {'Share':<10}")
    print("-" * 60)
    total_gain = df_imp['gain'].sum()
    for i, row in df_imp.head(20).iterrows():
        share = row['gain'] / total_gain
        print(f"{i+1:<5} {row['feature']:<30} {row['gain']:.2f}          {share:.1%}")
    print("-" * 60)

    # 4. ç”»å›¾ (å¸¦åå­—)
    print("\nğŸ“Š ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾è¡¨...")
    plt.figure(figsize=(12, 10))
    lgb.plot_importance(model, max_num_features=30, importance_type='gain',
                        height=0.5, title=f'Feature Importance (Gain) - End {year}', grid=False)
    plt.tight_layout()
    plt.savefig(Path(Config.OUTPUT_DIR) / f'feature_importance_{year}.png')

def plot_feature_importance(model, year):
    # (ä¿æŒä¸å˜ï¼Œå·²åºŸå¼ƒ)
    print("\nğŸ“Š ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾è¡¨...")
    plt.figure(figsize=(12, 10))
    lgb.plot_importance(model, max_num_features=30, importance_type='gain',
                        height=0.5, title=f'Feature Importance (Gain) - End {year}', grid=False)
    plt.tight_layout()
    plt.savefig(Path(Config.OUTPUT_DIR) / f'feature_importance_{year}.png')

def plot_history(evals, year):
    # (ä¿æŒä¸å˜)
    if not evals: return
    plt.figure(figsize=(10, 6))
    for k in ['binary_logloss', 'auc']:
        if k in evals.get('train', {}):
            plt.plot(evals['train'][k], label=f'Train {k}')
        if k in evals.get('val', {}):
            plt.plot(evals['val'][k], label=f'Val {k}')
    plt.title(f'Training Metrics - End {year}')
    plt.legend()
    plt.savefig(Path(Config.OUTPUT_DIR) / f'training_history_{year}.png')

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--end_year', type=int, default=2012)
    parser.add_argument('--gpu', action='store_true')
    args = parser.parse_args()

    train_dynamic(args.end_year, args.gpu)