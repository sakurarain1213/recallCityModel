
"""
åˆ†æ‰¹è®­ç»ƒè„šæœ¬ (Checkpoint & æé€Ÿä¼˜åŒ–ç‰ˆ)
åŠŸèƒ½: 
1. æ”¯æŒæ¯ N è½®ä¿å­˜ Checkpoint
2. ä½¿ç”¨ Mini-Validation Set åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°
3. ç§»é™¤è®­ç»ƒé›†å®æ—¶è¯„ä¼°ï¼Œå¤§å¹…æé€Ÿ
"""
import lightgbm as lgb
import pandas as pd
import gc
import time
import argparse
import os
from pathlib import Path
from src.config import Config
import matplotlib.pyplot as plt
import seaborn as sns

# æé€Ÿé…ç½®
FAST_PARAMS = {
    'objective': 'binary',
    'metric': ['binary_logloss', 'auc'],
    'boosting_type': 'goss',
    'top_rate': 0.2,
    'other_rate': 0.1,
    'num_leaves': 31,
    'max_depth': 8,
    'max_bin': 63,
    'learning_rate': 0.15,
    'n_estimators': 1000,
    'colsample_bytree': 0.8,
    'min_child_samples': 100,
    'lambda_l1': 0.1,
    'lambda_l2': 0.1,
    'n_jobs': 24,
    'verbosity': -1
}

def print_log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")

def load_data_batch(years, shuffle=True):
    """
    åŠ è½½æŒ‡å®šå¹´ä»½çš„æ•°æ®ä½œä¸ºä¸€ä¸ªBatch
    """
    dfs = []
    print_log(f"   ğŸ“¥ Loading parquet for years: {years}")
    for year in years:
        p = Path(Config.PROCESSED_DIR) / f"processed_{year}.parquet"
        if p.exists():
            df = pd.read_parquet(p)
            # ç®€å•çš„é˜²å¾¡æ€§ç±»å‹è½¬æ¢
            for c in ['From_City', 'To_City']:
                if c in df.columns: df[c] = df[c].astype('int16')
            if 'Label' in df.columns: df['Label'] = df['Label'].astype('float32')
            dfs.append(df)
    
    if not dfs: return None
    
    # åˆå¹¶
    df_batch = pd.concat(dfs, axis=0, ignore_index=True)
    
    # Batchå†…éƒ¨æ‰“ä¹±
    if shuffle:
        print_log(f"   ğŸ”€ Shuffling {len(df_batch):,} rows...")
        df_batch = df_batch.sample(frac=1, random_state=42).reset_index(drop=True)
        
    return df_batch

# === æ–°å¢ï¼šCheckpoint å›è°ƒå‡½æ•° ===
def save_checkpoint_callback(save_freq, output_dir, year_prefix):
    """
    æ¯ save_freq è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    """
    def callback(env):
        # env.iteration ä» 0 å¼€å§‹
        iteration = env.iteration + 1
        if iteration % save_freq == 0:
            # æ„é€ æ–‡ä»¶å: checkpoints/model_2010_round_50.txt
            ckpt_dir = Path(output_dir) / "checkpoints"
            ckpt_dir.mkdir(exist_ok=True)
            path = ckpt_dir / f"model_{year_prefix}_round_{iteration}.txt"
            env.model.save_model(str(path))
            print_log(f"   ğŸ’¾ Checkpoint saved: {path.name}")
    return callback

def train_batch_mode(target_end_year, batch_size_years=3, checkpoint_freq=50):
    total_start = time.time()
    print("="*60)
    print(f"ğŸš€ Batch Training Task: End Year {target_end_year}")
    print(f"ğŸ“¦ Batch Size: {batch_size_years} Years")
    print(f"â±ï¸ Checkpoint Frequency: Every {checkpoint_freq} rounds")
    print("="*60)

    # 1. è§„åˆ’ Batches
    all_train_years = list(range(2001, target_end_year - 2))
    val_years = [target_end_year - 2, target_end_year - 1]

    batches = [all_train_years[i:i + batch_size_years] for i in range(0, len(all_train_years), batch_size_years)]
    
    print_log(f"ğŸ“… Training Sequence: {batches}")
    print_log(f"ğŸ“… Validation Years: {val_years}")

    # 2. å‡†å¤‡éªŒè¯é›†
    print_log("\nğŸ“¦ Loading Validation Data (Global)...")
    df_val = load_data_batch(val_years, shuffle=False)
    
    # ã€æé€Ÿä¼˜åŒ–æ ¸å¿ƒã€‘
    # æ„é€ ä¸€ä¸ªæå°çš„éªŒè¯é›† (20ä¸‡) ä¸“é—¨ç”¨äº Early Stopping å’Œ å®æ—¶æ‰“å°
    # åŸå§‹ 200ä¸‡ å¤ªå¤§äº†ï¼Œæ¯è½®è¯„ä¼°å¤ªæ…¢
    WATCH_SIZE = 200000 
    
    if len(df_val) > WATCH_SIZE:
        print_log(f"âš¡ Creating Mini-Validation Set for Speed: {WATCH_SIZE:,} rows")
        # åˆ†ç¦»å‡º mini set
        df_val_watch = df_val.sample(n=WATCH_SIZE, random_state=42).reset_index(drop=True)
        # é‡Šæ”¾åŸå§‹å¤§è¡¨ (å¦‚æœå†…å­˜ç´§å¼ ) - æˆ–è€…ä¿ç•™ç”¨äºæœ€å Full Evaluate (è¿™é‡Œä¸ºäº†çœå†…å­˜å…ˆé‡Šæ”¾)
        del df_val
        gc.collect()
    else:
        df_val_watch = df_val
        del df_val

    # ç‰¹å¾è¯†åˆ«
    excludes = ['Year', 'From_City', 'To_City', 'Label', 'Rank', 'Flow_Count', 'qid']
    feats = [c for c in df_val_watch.columns if c not in excludes and not c.endswith('_orig')]
    cats = ['gender', 'age_group', 'education', 'industry', 'income', 'family', 'is_same_province']
    cats = [c for c in cats if c in feats]
    
    print_log(f"âœ¨ Features: {len(feats)} | Categorical: {len(cats)}")

    # æ„å»º Mini éªŒè¯é›† Dataset
    print_log("ğŸ”¨ Constructing Watch Dataset...")
    val_ds_watch = lgb.Dataset(
        df_val_watch[feats], 
        label=df_val_watch['Label'], 
        categorical_feature=cats, 
        params=FAST_PARAMS, 
        free_raw_data=False 
    )
    val_ds_watch.construct()
    
    # é‡Šæ”¾ Pandas å¯¹è±¡
    del df_val_watch
    gc.collect()

    # 3. å¾ªç¯è®­ç»ƒ (Incremental Learning)
    model = None
    
    for i, batch_years in enumerate(batches):
        print("\n" + "-"*40)
        print_log(f"ğŸ”„ Processing Batch {i+1}/{len(batches)}: Years {batch_years}")
        print("-"*40)
        
        # åŠ è½½ -> æ‰“ä¹±
        df_train = load_data_batch(batch_years, shuffle=True)
        if df_train is None: continue

        # ======================= ã€ç‰¹å¾å¯¹é½ã€‘ =======================
        # ğŸ›¡ï¸ ç¡®ä¿è®­ç»ƒé›†åŒ…å«æ‰€æœ‰éªŒè¯é›†æœ‰çš„ç‰¹å¾
        # å¦‚æœæ—©æœŸå¹´ä»½ç¼ºå°‘ 5y_avg ç­‰ç‰¹å¾ï¼Œæ‰‹åŠ¨è¡¥ä¸Šå¹¶å¡« -1
        missing_cols = [c for c in feats if c not in df_train.columns]
        if missing_cols:
            print_log(f"   âš ï¸ Aligning features: Filling {len(missing_cols)} missing cols (e.g., {missing_cols[0]}) with -1")
            for c in missing_cols:
                df_train[c] = -1.0
                df_train[c] = df_train[c].astype('float32')
        # ======================= ã€ç‰¹å¾å¯¹é½ç»“æŸã€‘ =======================

        print_log(f"   Rows: {len(df_train):,} | Memory: {df_train.memory_usage(deep=True).sum()/1024**3:.2f} GB")
        
        # æ„å»º Dataset
        t_build = time.time()
        train_ds = lgb.Dataset(
            df_train[feats], 
            label=df_train['Label'], 
            categorical_feature=cats, 
            params=FAST_PARAMS,
            free_raw_data=False 
        )
        train_ds.construct()
        print_log(f"   Dataset Built: {time.time()-t_build:.1f}s")
        
        del df_train
        gc.collect()
        
        # è®­ç»ƒ
        print_log("   ğŸ”¥ Training...")
        try:
            # å›è°ƒåˆ—è¡¨
            callbacks_list = [
                lgb.early_stopping(stopping_rounds=20, verbose=True),
                lgb.log_evaluation(50), # å‡å°‘æ‰“å°é¢‘ç‡åˆ° 50
                # æ·»åŠ  Checkpoint å›è°ƒ
                save_checkpoint_callback(checkpoint_freq, Config.OUTPUT_DIR, target_end_year)
            ]

            model = lgb.train(
                FAST_PARAMS,
                train_ds,
                num_boost_round=1000, 
                # ã€æé€Ÿå…³é”®ã€‘valid_sets åªæ”¾ mini éªŒè¯é›†ï¼Œä¸”ä¸æ”¾è®­ç»ƒé›†
                valid_sets=[val_ds_watch], 
                valid_names=['val_mini'], # æ”¹ä¸ªåå­—åŒºåˆ†
                init_model=model,            
                keep_training_booster=True,  
                callbacks=callbacks_list
            )
        except Exception as e:
            print_log(f"âŒ Training failed at batch {i+1}: {e}")
            raise e
        finally:
            del train_ds
            gc.collect()

    # 4. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    out_path = Path(Config.OUTPUT_DIR) / f'lgb_batch_end_{target_end_year}.txt'
    if model:
        model.save_model(str(out_path))
        print_log(f"\nâœ… All Batches Finished! Total time: {(time.time() - total_start)/60:.1f} min")
        print_log(f"ğŸ’¾ Model saved to: {out_path}")

        # ç»˜å›¾
        print("\n" + "="*40)
        print("ğŸ“Š Feature Importance (Gain)")
        print("="*40)
        
        importance = model.feature_importance(importance_type='gain')
        feature_names = model.feature_name()
        
        fi_df = pd.DataFrame({'feature': feature_names, 'importance': importance})
        fi_df = fi_df.sort_values(by='importance', ascending=False)
        
        print(fi_df.head(20).to_string(index=False))
        
        plt.figure(figsize=(12, 10))
        sns.barplot(x='importance', y='feature', data=fi_df.head(30))
        plt.title(f'LightGBM Feature Importance (Gain) - End Year {target_end_year}')
        plt.tight_layout()
        plt.savefig(Path(Config.OUTPUT_DIR) / f'feature_importance_{target_end_year}.png')
        print(f"\nğŸ–¼ï¸ Feature importance plot saved to output/feature_importance_{target_end_year}.png")
        
        geo_rank = fi_df[fi_df['feature'] == 'geo_distance'].index
        if len(geo_rank) > 0 and geo_rank[0] > 5:
            print("\nâš ï¸ è­¦å‘Š: åœ°ç†è·ç¦» (geo_distance) æƒé‡è¿‡ä½ï¼è¯·æ£€æŸ¥ city_edges æ˜¯å¦æ­£ç¡® Mergeï¼")

        print("="*40)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--end_year', type=int, default=2020, help='Target End Year')
    parser.add_argument('--batch_size', type=int, default=3, help='Years per batch')
    parser.add_argument('--ckpt_freq', type=int, default=50, help='Checkpoint frequency')
    args = parser.parse_args()
    
    train_batch_mode(args.end_year, args.batch_size, args.ckpt_freq)