import gc
import pandas as pd
import numpy as np
from pathlib import Path
import lightgbm as lgb
import matplotlib.pyplot as plt
import pickle
import argparse
from src.config import Config
# uv run train.py    ã€ä¸€å®šè¦åœ¨cmd ä¸è¦powershellã€‘
# uv run train.py --end_year 2010  ã€è®­ç»ƒåˆ°2010å¹´ åé¢å®Œå…¨ä¸ç”¨ã€‘
# uv run train.py --gpu  ã€ä½¿ç”¨ GPU è®­ç»ƒï¼Œéœ€è¦å…ˆå®‰è£… GPU ç‰ˆæœ¬ã€‘
# uv run train.py --end_year 2010 --gpu  ã€ç»„åˆä½¿ç”¨ã€‘
# uv run train.py --end_year 2020 --gpu  ã€ç»„åˆä½¿ç”¨ã€‘
# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False


def parse_year_config(end_year=None):
    """
    æ ¹æ®æˆªæ­¢å¹´ä»½è‡ªåŠ¨åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†

    Args:
        end_year: è®­ç»ƒæˆªæ­¢å¹´ä»½ï¼ˆåŒ…å«ï¼‰
                 - None: ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆè®­ç»ƒåˆ°2017ï¼ŒéªŒè¯2018ï¼Œæµ‹è¯•2019-2020ï¼‰
                 - 2010: è®­ç»ƒ2001-2007ï¼ŒéªŒè¯2008ï¼Œæµ‹è¯•2009-2010
                 - å…¶ä»–å¹´ä»½ä»¥æ­¤ç±»æ¨

    Returns:
        train_years, val_years, test_years
    """
    if end_year is None:
        # é»˜è®¤é…ç½®ï¼šä½¿ç”¨ parquet æ•°æ®åˆ° 2020
        train_years = list(range(Config.TRAIN_START_YEAR, Config.TRAIN_END_YEAR + 1))
        val_years = Config.VAL_YEARS
        test_years = Config.TEST_YEARS
        print(f"ä½¿ç”¨é»˜è®¤é…ç½®ï¼š")
        print(f"  è®­ç»ƒé›†: {train_years[0]}-{train_years[-1]} ({len(train_years)}å¹´)")
        print(f"  éªŒè¯é›†: {val_years}")
        print(f"  æµ‹è¯•é›†: {test_years}")
    else:
        # è‡ªå®šä¹‰é…ç½®ï¼šæ ¹æ® end_year è‡ªåŠ¨åˆ’åˆ†
        # è®­ç»ƒé›†ï¼š2001 åˆ° (end_year - 3)
        # éªŒè¯é›†ï¼šend_year - 2
        # æµ‹è¯•é›†ï¼šend_year - 1 åˆ° end_year

        if end_year < 2004:
            raise ValueError(f"end_year å¿…é¡» >= 2004ï¼ˆè‡³å°‘éœ€è¦3å¹´è®­ç»ƒæ•°æ® + 1å¹´éªŒè¯ + 2å¹´æµ‹è¯•ï¼‰")

        train_start = Config.TRAIN_START_YEAR  # 2001
        train_end = end_year - 3
        val_year = end_year - 2
        test_start = end_year - 1
        test_end = end_year

        train_years = list(range(train_start, train_end + 1))
        val_years = [val_year]
        test_years = list(range(test_start, test_end + 1))

        print(f"è‡ªå®šä¹‰é…ç½®ï¼ˆæˆªæ­¢å¹´ä»½={end_year}ï¼‰ï¼š")
        print(f"  è®­ç»ƒé›†: {train_years[0]}-{train_years[-1]} ({len(train_years)}å¹´)")
        print(f"  éªŒè¯é›†: {val_years}")
        print(f"  æµ‹è¯•é›†: {test_years}")

    return train_years, val_years, test_years


def load_processed_data(years, data_dir='output/processed_data'):
    """åŠ è½½å¤„ç†å¥½çš„parquetæ–‡ä»¶ï¼ˆç”Ÿæˆå™¨æ¨¡å¼ï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰"""
    for year in years:
        file_path = Path(data_dir) / f"processed_{year}.parquet"
        if file_path.exists():
            print(f"Loading {year}...")
            df = pd.read_parquet(file_path)
            yield df
            del df
            gc.collect()


def prepare_features(df):
    """å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾"""
    # æ’é™¤çš„åˆ— - ç§»é™¤æ³„éœ²ç‰¹å¾ã€IDåˆ—å’Œè¾…åŠ©åˆ—
    exclude_cols = [
        'Label',        # æ ‡ç­¾
        'To_City',      # ç›®æ ‡åŸå¸‚ï¼ˆä¸èƒ½ä½œä¸ºç‰¹å¾ï¼‰
        'Flow_Count',   # æ³„éœ²ï¼
        'Rank',         # æ³„éœ²ï¼
        'Total_Count',  # å¯èƒ½æ³„éœ²
        'pred_score',   # é¢„æµ‹ç»“æœ
        'Type_ID_orig', 'From_City_orig', # ä¸­é—´åˆ—

        # ã€å…³é”®ä¿®å¤ã€‘å¿…é¡»æ’é™¤ ID ç±»å’Œæ—¶é—´ç±»ç‰¹å¾ï¼
        'qid',          # ç»å¯¹ä¸èƒ½è¿›æ¨¡å‹ï¼Œè¿™æ˜¯éšæœºID
        'Year',         # å¹´ä»½ä¸å»ºè®®ç›´æ¥è¿›æ ‘æ¨¡å‹ï¼ˆå¤–æ¨æ€§å·®ï¼‰ï¼Œé™¤éåšæˆç±»åˆ«
        'Month'         # å¦‚æœMonthéƒ½æ˜¯12ï¼Œä¹Ÿæ²¡æ„ä¹‰
    ]

    # ç‰¹å¾åˆ—
    feature_cols = [col for col in df.columns if col not in exclude_cols]

    X = df[feature_cols]
    y = df['Label']

    return X, y, feature_cols


def calculate_ndcg(y_true, y_pred, group_counts, k=20):
    """è®¡ç®—NDCG@kï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†äºŒåˆ†ç±»æ¨¡å¼ä¸‹ä¸ä½¿ç”¨ï¼‰"""
    from sklearn.metrics import ndcg_score

    ndcg_scores = []
    start_idx = 0

    for count in group_counts:
        end_idx = start_idx + count
        y_t = y_true[start_idx:end_idx]
        y_p = y_pred[start_idx:end_idx]

        # è®¡ç®—NDCG
        if len(y_t) > 0:
            score = ndcg_score([y_t], [y_p], k=k)
            ndcg_scores.append(score)

        start_idx = end_idx

    return np.mean(ndcg_scores)


def train_model(train_years=None, val_years=None, use_gpu=False):
    """è®­ç»ƒäºŒåˆ†ç±»æ¨¡å‹ï¼ˆä½¿ç”¨åˆ†æ‰¹åŠ è½½é¿å…å†…å­˜æº¢å‡ºï¼‰

    Args:
        train_years: è®­ç»ƒå¹´ä»½åˆ—è¡¨ï¼Œå¦‚ [2001, 2002, ..., 2017]
        val_years: éªŒè¯å¹´ä»½åˆ—è¡¨ï¼Œå¦‚ [2018]
        use_gpu: æ˜¯å¦ä½¿ç”¨ GPU è®­ç»ƒï¼ˆéœ€è¦å®‰è£… lightgbm GPU ç‰ˆæœ¬ï¼‰
    """
    if train_years is None:
        train_years = list(range(Config.TRAIN_START_YEAR, Config.TRAIN_END_YEAR + 1))
    if val_years is None:
        val_years = Config.VAL_YEARS

    print("="*60)
    print(f"Step 1: Preparing training data ({train_years[0]}-{train_years[-1]})...")
    if use_gpu:
        print("ğŸš€ GPU è®­ç»ƒæ¨¡å¼å·²å¯ç”¨")
    print("="*60)

    # åˆ†æ‰¹ç­–ç•¥ï¼šæ¯3å¹´ä¸€æ‰¹ï¼Œé¿å…å†…å­˜æº¢å‡ºåŒæ—¶ä¿è¯å­¦ä¹ æ•ˆæœ
    # === ä¿®æ”¹å ===
    import random
    # å½»åº•æ‰“ä¹±å¹´ä»½ï¼Œæ‰“ç ´æ—¶é—´ä¾èµ–
    # ä¾‹å¦‚ï¼šBatch 1 å¯èƒ½æ˜¯ [2015, 2002, 2010]
    # è¿™æ ·æ¨¡å‹æ¯ä¸€æ‰¹éƒ½èƒ½çœ‹åˆ°ä¸åŒæ—¶ä»£çš„ç‰¹å¾ï¼Œä¸ä¼š"é—å¿˜"å¤ä»£ï¼Œä¹Ÿä¸ä¼š"è¿‡æ‹Ÿåˆ"ç°ä»£
    shuffled_years = train_years.copy()
    random.shuffle(shuffled_years)

    batch_size = 3
    year_batches = [shuffled_years[i:i+batch_size] for i in range(0, len(shuffled_years), batch_size)]

    print(f"Training strategy: {len(year_batches)} batches (Randomized Years)")
    for i, batch in enumerate(year_batches):
        print(f"  Batch {i+1}: Years {batch}")

    # ç¬¬ä¸€æ­¥ï¼šåŠ è½½ç¬¬ä¸€æ‰¹æ•°æ®ä»¥è·å–ç‰¹å¾åˆ—
    print(f"\nLoading first batch to extract feature columns...")
    first_year_file = Path('output/processed_data') / f"processed_{train_years[0]}.parquet"
    first_df = pd.read_parquet(first_year_file)
    _, _, feature_cols = prepare_features(first_df)
    print(f"Features: {len(feature_cols)}")
    del first_df
    gc.collect()

    print("\n" + "="*60)
    print(f"Step 2: Loading validation data ({val_years[0]})...")
    print("="*60)

    val_file = Path('output/processed_data') / f"processed_{val_years[0]}.parquet"
    val_df = pd.read_parquet(val_file)
    print(f"Validation data: {len(val_df):,} rows")

    X_val, y_val, _ = prepare_features(val_df)

    del val_df
    gc.collect()

    print("\n" + "="*60)
    print("Step 3: Training Binary Classification model (batch mode)...")
    print("="*60)

    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    output_dir = Path(Config.OUTPUT_DIR) / 'models'
    output_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_dir = output_dir / 'checkpoints'
    checkpoint_dir.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒå‚æ•°
    params = Config.LGBM_PARAMS_GPU.copy() if use_gpu else Config.LGBM_PARAMS.copy()

    # åˆå§‹åŒ–æ¨¡å‹
    current_booster = None

    # åˆ†æ‰¹è®­ç»ƒ
    for batch_idx, year_batch in enumerate(year_batches):
        print(f"\n{'='*60}")
        print(f"Batch {batch_idx+1}/{len(year_batches)}: Years {year_batch[0]}-{year_batch[-1]}")
        print(f"{'='*60}")

        # åŠ è½½è¯¥æ‰¹æ¬¡çš„æ‰€æœ‰å¹´ä»½æ•°æ®
        batch_dfs = []
        for year in year_batch:
            year_file = Path('output/processed_data') / f"processed_{year}.parquet"
            if not year_file.exists():
                print(f"  Warning: {year_file} not found, skipping")
                continue

            print(f"  Loading year {year}...")
            year_df = pd.read_parquet(year_file)
            batch_dfs.append(year_df)

        if not batch_dfs:
            print(f"  No data found for batch {batch_idx+1}, skipping")
            continue

        # åˆå¹¶è¯¥æ‰¹æ¬¡çš„æ•°æ®
        print(f"  Merging {len(batch_dfs)} years...")
        batch_df = pd.concat(batch_dfs, axis=0, ignore_index=True)
        print(f"  Total rows in batch: {len(batch_df):,}")

        del batch_dfs
        gc.collect()

        # å‡†å¤‡ç‰¹å¾
        X_train, y_train, _ = prepare_features(batch_df)

        del batch_df
        gc.collect()

        # åˆ›å»ºè®­ç»ƒé›†ï¼ˆäºŒåˆ†ç±»ä¸éœ€è¦ group å‚æ•°ï¼‰
        print(f"  Creating LightGBM Dataset (Binary Mode)...")
        train_data = lgb.Dataset(X_train, label=y_train, free_raw_data=False)

        # åˆ›å»ºéªŒè¯é›†ï¼ˆreference è®¾ä¸ºå½“å‰è®­ç»ƒé›†ï¼Œä¿è¯ç‰¹å¾åˆ†æ¡¶ä¸€è‡´ï¼‰
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data, free_raw_data=False)

        # è®­ç»ƒï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€æ‰¹ï¼Œåˆ›å»ºæ–°æ¨¡å‹ï¼›å¦åˆ™ç»§ç»­è®­ç»ƒï¼‰
        if current_booster is None:
            print(f"  Creating new model...")
            current_booster = lgb.train(
                params,
                train_data,
                valid_sets=[val_data],
                valid_names=['valid'],
                num_boost_round=200,  # æ¯æ‰¹è®­ç»ƒ200è½®
                callbacks=[
                    lgb.log_evaluation(period=50),  # æ¯50è½®æ‰“å°ä¸€æ¬¡
                    lgb.early_stopping(stopping_rounds=30, verbose=False),  # 30è½®ä¸æå‡åˆ™æ—©åœ
                ],
                keep_training_booster=True  # å…è®¸ç»§ç»­è®­ç»ƒ
            )
        else:
            print(f"  Continuing training from previous model (current trees: {current_booster.num_trees()})...")
            current_booster = lgb.train(
                params,
                train_data,
                valid_sets=[val_data],
                valid_names=['valid'],
                num_boost_round=200,  # æ¯æ‰¹è®­ç»ƒ200è½®
                init_model=current_booster,  # ä»ä¸Šä¸€æ‰¹çš„æ¨¡å‹ç»§ç»­
                callbacks=[
                    lgb.log_evaluation(period=50),  # æ¯50è½®æ‰“å°ä¸€æ¬¡
                    lgb.early_stopping(stopping_rounds=30, verbose=False),  # 30è½®ä¸æå‡åˆ™æ—©åœ
                ],
                keep_training_booster=True  # å…è®¸ç»§ç»­è®­ç»ƒ
            )

        del X_train, y_train, train_data
        gc.collect()

        print(f"  Batch {batch_idx+1} completed.")
        print(f"  Total trees in model: {current_booster.num_trees()}")
        # æ‰“å° binary_logloss
        if 'binary_logloss' in current_booster.best_score['valid']:
            print(f"  Current best Binary LogLoss: {current_booster.best_score['valid']['binary_logloss']:.6f}")

        # ä¿å­˜ä¸­é—´ checkpoint
        checkpoint_path = checkpoint_dir / f'model_batch_{batch_idx+1}_years_{year_batch[0]}-{year_batch[-1]}.txt'
        current_booster.save_model(str(checkpoint_path))
        print(f"  Checkpoint saved: {checkpoint_path}")

    print("\n" + "="*60)
    print("Step 4: Final evaluation on validation set...")
    print("="*60)

    # æ‰“å° binary_logloss
    if 'binary_logloss' in current_booster.best_score['valid']:
        print(f"Best Validation Binary LogLoss: {current_booster.best_score['valid']['binary_logloss']:.6f}")
    print(f"Total trees in final model: {current_booster.num_trees()}")

    print("\n" + "="*60)
    print("Step 5: Saving model...")
    print("="*60)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model_path = output_dir / 'binary_model.txt'
    current_booster.save_model(str(model_path))
    print(f"Model saved to {model_path}")

    # ä¿å­˜ç‰¹å¾åˆ—å
    feature_path = output_dir / 'feature_cols.pkl'
    with open(feature_path, 'wb') as f:
        pickle.dump(feature_cols, f)
    print(f"Feature columns saved to {feature_path}")

    return current_booster, feature_cols


def plot_feature_importance(model, feature_cols, top_n=20):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
    print("\n" + "="*60)
    print("Step 7: Plotting feature importance...")
    print("="*60)

    # è·å–ç‰¹å¾é‡è¦æ€§
    importance = model.feature_importance(importance_type='gain')
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': importance
    }).sort_values('importance', ascending=False)

    print(f"\nTop {top_n} features:")
    print(feature_importance.head(top_n))

    # ç»˜å›¾
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(top_n)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Feature Importance (Gain)')
    plt.title(f'Top {top_n} Feature Importance')
    plt.gca().invert_yaxis()
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    output_dir = Path(Config.OUTPUT_DIR) / 'plots'
    output_dir.mkdir(parents=True, exist_ok=True)
    plot_path = output_dir / 'feature_importance.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    print(f"\nFeature importance plot saved to {plot_path}")

    plt.close()

    return feature_importance


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è®­ç»ƒè¿ç§»æ’åºæ¨¡å‹')
    parser.add_argument(
        '--end_year',
        type=int,
        default=None,
        help='è®­ç»ƒæˆªæ­¢å¹´ä»½ï¼ˆåŒ…å«ï¼‰ã€‚ä¸å¡«åˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆè®­ç»ƒåˆ°2017ï¼‰ã€‚'
             'ä¾‹å¦‚ï¼š--end_year 2010 è¡¨ç¤ºè®­ç»ƒ2001-2007ï¼ŒéªŒè¯2008ï¼Œæµ‹è¯•2009-2010'
    )
    parser.add_argument(
        '--gpu',
        action='store_true',
        help='ä½¿ç”¨ GPU è®­ç»ƒï¼ˆéœ€è¦å®‰è£… lightgbm GPU ç‰ˆæœ¬å’Œ CUDAï¼‰'
    )
    args = parser.parse_args()

    # æ ¹æ®å‚æ•°é…ç½®å¹´ä»½åˆ’åˆ†
    print("="*60)
    print("å¹´ä»½é…ç½®")
    print("="*60)
    train_years, val_years, test_years = parse_year_config(args.end_year)
    print()

    # è®­ç»ƒæ¨¡å‹
    model, feature_cols = train_model(train_years, val_years, use_gpu=args.gpu)

    # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
    feature_importance = plot_feature_importance(model, feature_cols)

    print("\n" + "="*60)
    print("Training completed!")
    print("="*60)
