"""
æé€Ÿè¯„ä¼°è„šæœ¬ (å®½è¡¨é€‚é…ä¿®å¤ç‰ˆ)
åŠŸèƒ½: åŠ è½½æ¨¡å‹ -> ä»å®½è¡¨ DB è¯»å–å¹¶å±•å¼€ GT -> æ„é€ å…¨é‡å€™é€‰é›† -> ç‰¹å¾å·¥ç¨‹ -> é¢„æµ‹ -> è®¡ç®— Recall
ä¿®å¤: è§£å†³äº† DuckDB è¡¨ç»“æ„ä¸åŒ¹é…çš„é—®é¢˜ (Rank åˆ—ä¸å­˜åœ¨)
"""
import gc
import pandas as pd
import numpy as np
import lightgbm as lgb
import duckdb
import argparse
import re
from pathlib import Path
from src.config import Config
from src.city_data import CityDataLoader
from src.feature_pipeline import FeaturePipeline

# ------------------------------------------------------------------------------
# 1. æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼šè¯»å–å®½è¡¨å¹¶è½¬ä¸ºé•¿è¡¨ GT
# ------------------------------------------------------------------------------
def load_ground_truth(db_path, year):
    """
    ä» DuckDB è¯»å–å®½è¡¨æ•°æ®ï¼Œå¹¶å±•å¼€ä¸ºé•¿è¡¨æ ¼å¼ (Year, Type, From, To, Rank)
    """
    print(f"ğŸ“¥ Querying DuckDB for year {year} (Wide Table)...")
    
    con = duckdb.connect(str(db_path), read_only=True)
    
    # 1. æ„é€  SQL æŸ¥è¯¢ Top 20 çš„åˆ—
    # æˆ‘ä»¬éœ€è¦ Year, Type_ID, From_City ä»¥åŠæ‰€æœ‰çš„ To_TopX å’Œ To_TopX_Count
    top_cols = []
    for i in range(1, 21):
        top_cols.append(f"To_Top{i}")
        # top_cols.append(f"To_Top{i}_Count") # å…¶å®è¯„ä¼°åªéœ€è¦çŸ¥é“å»å“ªäº†ï¼ŒCount å¯é€‰
    
    cols_str = ", ".join(top_cols)
    
    query = f"""
    SELECT 
        Year, 
        Type_ID, 
        From_City, 
        {cols_str}
    FROM migration_data
    WHERE Year = {year}
    """
    
    try:
        df_wide = con.execute(query).df()
        if df_wide.empty:
            return pd.DataFrame()
            
        print(f"   âœ“ Loaded {len(df_wide):,} wide rows. Unpivoting to long format...")
        
        # 2. æ¸…æ´— From_City (å»é™¤ä¸­æ–‡ï¼Œåªç•™ ID)
        # å‡è®¾ From_City å¯èƒ½æ˜¯ "æ·±åœ³(4403)" è¿™ç§æ ¼å¼
        if df_wide['From_City'].dtype == 'object':
             df_wide['From_City'] = df_wide['From_City'].astype(str).str.extract(r'(\d+)', expand=False)
        df_wide['From_City'] = pd.to_numeric(df_wide['From_City'], errors='coerce').fillna(0).astype('int16')

        # 3. å®½è¡¨è½¬é•¿è¡¨ (Melt)
        # id_vars = [Year, Type_ID, From_City]
        # value_vars = [To_Top1, ..., To_Top20]
        df_long = pd.melt(
            df_wide, 
            id_vars=['Year', 'Type_ID', 'From_City'], 
            value_vars=[f"To_Top{i}" for i in range(1, 21)],
            var_name='Rank_Str', 
            value_name='To_City_Raw'
        )
        
        # 4. è§£æ Rank å’Œ To_City
        # Rank_Str æ˜¯ "To_Top1", "To_Top2"... -> æå–æ•°å­—ä½œä¸º Rank
        df_long['Rank'] = df_long['Rank_Str'].str.extract(r'(\d+)').astype(int).astype('int16')
        
        # To_City_Raw å¯èƒ½æ˜¯ "ä¸Šæµ·(3100)" æˆ– "0" æˆ– None
        # æˆ‘ä»¬éœ€è¦æå–å…¶ä¸­çš„æ•°å­— ID
        df_long = df_long.dropna(subset=['To_City_Raw'])
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¤„ç†
        df_long['To_City_Raw'] = df_long['To_City_Raw'].astype(str)
        # æå–æ•°å­— (å¦‚æœæœ¬æ¥å°±æ˜¯æ•°å­—å­—ç¬¦ä¸²ä¹Ÿèƒ½æå–)
        df_long['To_City'] = df_long['To_City_Raw'].str.extract(r'(\d+)', expand=False)
        # è½¬ä¸ºæ•°å­—ï¼Œéæ•°å­—å˜ä¸º NaN
        df_long['To_City'] = pd.to_numeric(df_long['To_City'], errors='coerce')
        
        # 5. è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        # å»é™¤ To_City ä¸º 0 æˆ– NaN çš„è¡Œ (è¡¨ç¤ºæ²¡æœ‰ TopX æ•°æ®)
        # ä¹Ÿè¦å»é™¤ To_City == From_City çš„è¡Œ (è™½ç„¶ç†è®ºä¸Š Top ä¸åº”è¯¥åŒ…å«è‡ªå·±)
        df_valid = df_long[
            (df_long['To_City'].notna()) & 
            (df_long['To_City'] > 0)
        ].copy()
        
        df_valid['To_City'] = df_valid['To_City'].astype('int16')
        
        # åªä¿ç•™éœ€è¦çš„åˆ—
        final_df = df_valid[['Year', 'Type_ID', 'From_City', 'To_City', 'Rank']].reset_index(drop=True)
        
        return final_df

    except Exception as e:
        print(f"âŒ DB Error: {e}")
        return pd.DataFrame()
    finally:
        con.close()

# ------------------------------------------------------------------------------
# 2. æ ¸å¿ƒè¯„ä¼°é€»è¾‘
# ------------------------------------------------------------------------------
def run_main(year, model_path, sample_size):
    # 1. åŠ è½½èµ„æº
    loader = CityDataLoader(Config.DATA_DIR).load_all()
    pipeline = FeaturePipeline(loader, data_dir=Config.PROCESSED_DIR)
    
    if not Path(model_path).exists():
        print(f"âŒ Model not found: {model_path}")
        return
    model = lgb.Booster(model_file=model_path)
    model_feats = model.feature_name()
    print(f"âœ… Model loaded: {model_path} ({len(model_feats)} feats)")

    # 2. è·å– GT (ä½¿ç”¨æ–°å‡½æ•°)
    df_true = load_ground_truth(Config.DB_PATH, year)
    
    if df_true.empty:
        print("âŒ No ground truth data found.")
        return
        
    print(f"   âœ“ Extracted {len(df_true):,} valid ground truth pairs (Rank <= 20)")

    # æå– Queries (Year, Type_ID, From_City)
    queries = df_true[['Year', 'Type_ID', 'From_City']].drop_duplicates()
    
    if sample_size and len(queries) > sample_size:
        print(f"âš¡ Sampling {sample_size} queries from {len(queries)}...")
        queries = queries.sample(n=sample_size, random_state=42)
    else:
        print(f"ğŸ“Š Evaluating {len(queries)} queries...")
    
    # 3. æ„é€ å€™é€‰é›† (Query * All_Cities)
    print("ğŸ”¨ Generating Candidates...")
    all_cities = loader.get_city_ids()
    
    # ç¬›å¡å°”ç§¯
    queries = queries.copy()
    queries['key'] = 1
    targets = pd.DataFrame({'To_City': all_cities, 'key': 1})
    candidates = pd.merge(queries, targets, on='key').drop('key', axis=1)
    
    # æ’é™¤ From == To
    candidates = candidates[candidates['From_City'] != candidates['To_City']].copy()
    
    # 4. ç‰¹å¾å·¥ç¨‹
    print("âœ¨ Feature Engineering...")
    # ä¸ºäº†å¤ç”¨ pipelineï¼Œéœ€è¦ Flow_Count å ä½
    candidates['Flow_Count'] = 0 
    
    # Pipeline å˜æ¢ (ç”Ÿæˆç‰¹å¾)
    df_feats = pipeline.transform(candidates.copy(), year, mode='eval', verbose=False)
    
    # ç±»å‹å¤„ç† (ä¸è®­ç»ƒä¸€è‡´)
    if 'Type_ID' in df_feats.columns and df_feats['Type_ID'].dtype == 'object':
        df_feats['Type_Hash'] = pd.util.hash_pandas_object(df_feats['Type_ID'], index=False).astype('int64')
        df_feats.drop(columns=['Type_ID'], inplace=True)
    
    # å‡†å¤‡ X (ç‰¹å¾çŸ©é˜µ)
    X = pd.DataFrame(index=df_feats.index)
    for f in model_feats:
        if f in df_feats.columns:
            X[f] = df_feats[f]
        else:
            X[f] = 0
            
    # è½¬ float32
    for c in X.columns:
        if X[c].dtype == 'float64': X[c] = X[c].astype('float32')

    # 5. é¢„æµ‹
    print("ğŸ”® Predicting...")
    # å°†é¢„æµ‹åˆ†æ•°èµ‹å€¼å› candidates (ç”¨äºåç»­æ’åº)
    candidates['score'] = model.predict(X)
    
    # 6. è®¡ç®—æŒ‡æ ‡ (Recall@K)
    print("ğŸ“‰ Calculating Metrics...")
    
    # 6.1 æ„é€ å¿«é€ŸæŸ¥æ‰¾çš„ GT é›†åˆ
    # æ ¼å¼: (Type_ID, From_City, To_City) -> True
    gt_set = set(zip(df_true['Type_ID'], df_true['From_City'], df_true['To_City']))
    
    # 6.2 æ’åº: å¯¹æ¯ä¸ª (Type_ID, From_City) åˆ†ç»„ï¼ŒæŒ‰åˆ†æ•°é™åºæ’åˆ—
    candidates['rank'] = candidates.groupby(['Type_ID', 'From_City'])['score'].rank(method='first', ascending=False)
    
    # 6.3 åªä¿ç•™ Top 20 é¢„æµ‹ç»“æœè¿›è¡Œç»Ÿè®¡
    top_preds = candidates[candidates['rank'] <= 20].copy()
    
    # 6.4 åˆ¤æ–­æ˜¯å¦å‘½ä¸­ GT
    top_preds['is_hit'] = top_preds.apply(lambda x: (x['Type_ID'], x['From_City'], x['To_City']) in gt_set, axis=1)
    
    # 6.5 èšåˆç»Ÿè®¡
    hits = top_preds.groupby(['Type_ID', 'From_City']).apply(
        lambda x: pd.Series({
            'hit_1': x[x['rank'] <= 1]['is_hit'].sum(),
            'hit_5': x[x['rank'] <= 5]['is_hit'].sum(),
            'hit_10': x[x['rank'] <= 10]['is_hit'].sum(),
            'hit_20': x[x['rank'] <= 20]['is_hit'].sum()
        })
    ).reset_index()
    
    # è·å–æ¯ä¸ª Query å¯¹åº”çš„çœŸå®æµå‘æ€»æ•° (åˆ†æ¯)
    gt_counts = df_true.groupby(['Type_ID', 'From_City']).size().reset_index(name='total_true')
    
    # åˆå¹¶
    res = pd.merge(hits, gt_counts, on=['Type_ID', 'From_City'], how='left').fillna(0)
    
    # è®¡ç®—å¹³å‡ Recall
    res['total_true'] = res['total_true'].replace(0, 1)
    
    r1 = (res['hit_1'] / res['total_true']).mean()
    r5 = (res['hit_5'] / res['total_true']).mean()
    r10 = (res['hit_10'] / res['total_true']).mean()
    r20 = (res['hit_20'] / res['total_true']).mean()
    
    print("\n" + "="*40)
    print(f"ğŸ“Š Evaluation Results for Year {year}")
    print("="*40)
    print(f"Queries Evaluated : {len(res)}")
    print(f"Avg GT per Query: {res['total_true'].mean():.2f}")
    print("-" * 30)
    print(f"Recall@1  : {r1:.2%}")
    print(f"Recall@5  : {r5:.2%}")
    print(f"Recall@10 : {r10:.2%}")
    print(f"Recall@20 : {r20:.2%}")
    print("="*40)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--year', type=int, default=2018, help="Year to evaluate")
    parser.add_argument('--model', type=str, default=None, help="Path to model file")
    parser.add_argument('--sample', type=int, default=1000, help="Number of queries to sample (speed up)")
    args = parser.parse_args()
    
    # è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹
    if args.model is None:
        p = Path(Config.OUTPUT_DIR) / f"lgb_batch_end_{args.year}.txt"
        if not p.exists():
             models = list(Path(Config.OUTPUT_DIR).glob("lgb_batch_end_*.txt"))
             if models:
                 p = max(models, key=lambda f: f.stat().st_mtime)
        args.model = str(p)

    run_main(args.year, args.model, args.sample)