"""
æé€Ÿè¯„ä¼°è„šæœ¬ (å®½è¡¨é€‚é…ä¿®å¤ç‰ˆ)
åŠŸèƒ½: åŠ è½½æ¨¡å‹ -> ä»å®½è¡¨ DB è¯»å–å¹¶å±•å¼€ GT -> æ„é€ å…¨é‡å€™é€‰é›† -> ç‰¹å¾å·¥ç¨‹ -> é¢„æµ‹ -> è®¡ç®— Recall
ä¿®å¤: è§£å†³äº† DuckDB è¡¨ç»“æ„ä¸åŒ¹é…çš„é—®é¢˜ (Rank åˆ—ä¸å­˜åœ¨)
ä¿®å¤: è§£å†³äº† ID ç±»å‹ä¸åŒ¹é…å¯¼è‡´çš„ 0% Recall é—®é¢˜
"""
import gc
import pandas as pd
import numpy as np
import lightgbm as lgb
import duckdb
import argparse
import re
from pathlib import Path
from src.config import Config
from src.city_data import CityDataLoader
from src.feature_pipeline import FeaturePipeline
from src.feature_eng import extract_city_id  # å¤ç”¨åŒä¸€ä¸ªæå–å‡½æ•°

# ------------------------------------------------------------------------------
# 1. æ ¸å¿ƒè¾…åŠ©å‡½æ•°ï¼šè¯»å–å®½è¡¨å¹¶è½¬ä¸ºé•¿è¡¨ GT
# ------------------------------------------------------------------------------
def load_ground_truth(db_path, year):
    """
    ä¿®å¤ç‰ˆï¼šä¸¥æ ¼æå– Int ç±»å‹çš„ ID  å°¤å…¶æ³¨æ„ground truthæ˜¯ top10 è€Œä¸æ˜¯20ä¸ªéƒ½åŠ è½½è¿›æ¥ã€‚
    """
    print(f"ğŸ“¥ Querying DuckDB for year {year}...")
    con = duckdb.connect(str(db_path), read_only=True)

    # æŸ¥è¯¢
    # ä¿®æ”¹å: åªåŠ è½½ Top 10 ä½œä¸º Ground Truth
    # è¿™æ ·åˆ†æ¯(GTæ€»æ•°)å°±å˜æˆäº† 10ï¼ŒRecall@10 çš„ç†è®ºä¸Šé™å°±æ˜¯ 100% äº†
    top_cols = [f"To_Top{i}" for i in range(1, 11)] 
    cols_str = ", ".join(top_cols)

    query = f"SELECT Year, Type_ID, From_City, {cols_str} FROM migration_data WHERE Year = {year}"

    try:
        df_wide = con.execute(query).df()
        if df_wide.empty: return pd.DataFrame()

        # 1. æ¸…æ´— From_City (è½¬ Int)
        df_wide['From_City'] = df_wide['From_City'].apply(extract_city_id).astype('int16')

        # 2. Melt
        df_long = pd.melt(df_wide, id_vars=['Year', 'Type_ID', 'From_City'],
                          value_vars=top_cols, value_name='To_City_Raw')

        # 3. æ¸…æ´— To_City (è½¬ Int)
        # è¿™ä¸€æ­¥æ˜¯ä¹‹å‰çš„ç—›ç‚¹ï¼šTo_Top1 å¯èƒ½æ˜¯ "æˆéƒ½(5101)"
        df_long = df_long.dropna(subset=['To_City_Raw'])
        df_long['To_City'] = df_long['To_City_Raw'].apply(extract_city_id)

        # è¿‡æ»¤æ— æ•ˆID
        df_long = df_long[df_long['To_City'] > 0].copy()
        df_long['To_City'] = df_long['To_City'].astype('int16')

        # å»é‡ (åŒä¸€Queryå¯èƒ½å¤šä¸ªRankæŒ‡å‘åŒä¸€åŸå¸‚? ä¸€èˆ¬ä¸ä¼šï¼Œä½†é˜²ä¸‡ä¸€)
        final_df = df_long[['Year', 'Type_ID', 'From_City', 'To_City']].drop_duplicates()

        return final_df
    except Exception as e:
        print(f"âŒ DB Error: {e}")
        return pd.DataFrame()
    finally:
        con.close()

# ------------------------------------------------------------------------------
# 2. æ ¸å¿ƒè¯„ä¼°é€»è¾‘
# ------------------------------------------------------------------------------

def run_main(year, model_path, sample_size):
    # 1. åŠ è½½èµ„æº (CityData ä¼šå¼ºåˆ¶ ID ä¸º Int)
    loader = CityDataLoader(Config.DATA_DIR).load_all()
    pipeline = FeaturePipeline(loader, data_dir=Config.PROCESSED_DIR)

    # ã€å…³é”®ä¿®æ”¹ã€‘ è·¯å¾„æ£€æŸ¥
    if not model_path:
        print("âŒ Error: No model path provided.")
        return

    path_obj = Path(model_path)
    if not path_obj.exists():
        print(f"âŒ Error: Model file does not exist at: {model_path}")
        return
        
    print(f"ğŸ“‚ Loading Model from: {path_obj.absolute()}")
    model = lgb.Booster(model_file=str(path_obj)) # ç¡®ä¿è½¬ä¸º str
    model_feats = model.feature_name()
    print(f"âœ… Model loaded successfully ({len(model_feats)} feats)")

    # 2. è·å– GT (ç°åœ¨å…¨æ˜¯ Int)
    df_true = load_ground_truth(Config.DB_PATH, year)
    if df_true.empty:
        print("âŒ No ground truth data found.")
        return

    print(f"   âœ“ Extracted {len(df_true):,} valid ground truth pairs")

    # é‡‡æ ·
    queries = df_true[['Year', 'Type_ID', 'From_City']].drop_duplicates()
    if sample_size and len(queries) > sample_size:
        print(f"âš¡ Sampling {sample_size} queries from {len(queries)}...")
        queries = queries.sample(n=sample_size, random_state=42)
    else:
        print(f"ğŸ“Š Evaluating {len(queries)} queries...")

    # 3. æ„é€ å€™é€‰é›† (å€™é€‰ ID å¿…é¡»æ˜¯ Int)
    print("ğŸ”¨ Generating Candidates...")
    all_cities = loader.get_city_ids()  # è¿™æ˜¯ä¸€ä¸ª Int List
    
    # ç¬›å¡å°”ç§¯
    queries = queries.copy()
    queries['key'] = 1
    targets = pd.DataFrame({'To_City': all_cities, 'key': 1})  # To_City æ˜¯ Int
    candidates = pd.merge(queries, targets, on='key').drop('key', axis=1)
    
    # æ’é™¤ From == To
    candidates = candidates[candidates['From_City'] != candidates['To_City']].copy()
    
    # 4. ç‰¹å¾å·¥ç¨‹
    print("âœ¨ Feature Engineering...")
    candidates['Flow_Count'] = 0
    df_feats = pipeline.transform(candidates.copy(), year, mode='predict', verbose=False)

    # å‡†å¤‡ X
    X = pd.DataFrame(index=df_feats.index)
    for f in model_feats:
        X[f] = df_feats[f] if f in df_feats.columns else 0.0

    # è½¬ float32
    for c in X.columns:
        if X[c].dtype == 'float64': X[c] = X[c].astype('float32')

    # 5. é¢„æµ‹
    print("ğŸ”® Predicting...")
    scores = model.predict(X)
    candidates['score'] = scores

    # 6. è®¡ç®—æŒ‡æ ‡
    print("ğŸ“‰ Calculating Metrics...")
    gt_set = set(zip(df_true['Type_ID'], df_true['From_City'], df_true['To_City']))
    
    candidates['rank'] = candidates.groupby(['Type_ID', 'From_City'])['score'].rank(method='first', ascending=False)
    top_preds = candidates[candidates['rank'] <= 20].copy()
    
    top_preds['is_hit'] = top_preds.apply(lambda x: (x['Type_ID'], x['From_City'], x['To_City']) in gt_set, axis=1)
    
    hits = top_preds.groupby(['Type_ID', 'From_City']).apply(
        lambda x: pd.Series({
            'hit_1': x[x['rank'] <= 1]['is_hit'].sum(),
            'hit_5': x[x['rank'] <= 5]['is_hit'].sum(),
            'hit_10': x[x['rank'] <= 10]['is_hit'].sum(),
            'hit_20': x[x['rank'] <= 20]['is_hit'].sum()
        })
    ).reset_index()
    
    gt_counts = df_true.groupby(['Type_ID', 'From_City']).size().reset_index(name='total_true')
    res = pd.merge(hits, gt_counts, on=['Type_ID', 'From_City'], how='left').fillna(0)
    res['total_true'] = res['total_true'].replace(0, 1)
    
    r1 = (res['hit_1'] / res['total_true']).mean()
    r5 = (res['hit_5'] / res['total_true']).mean()
    r10 = (res['hit_10'] / res['total_true']).mean()
    r20 = (res['hit_20'] / res['total_true']).mean()
    
    print("\n" + "="*40)
    print(f"ğŸ“Š Evaluation Results for Year {year}")
    print(f"ğŸ¤– Model: {Path(model_path).name}")
    print("="*40)
    print(f"Queries Evaluated : {len(res)}")
    print(f"Avg GT per Query: {res['total_true'].mean():.2f}")
    print("-" * 30)
    print(f"Recall@1  : {r1:.2%}")
    print(f"Recall@5  : {r5:.2%}")
    print(f"Recall@10 : {r10:.2%}")
    print(f"Recall@20 : {r20:.2%}")
    print("="*40)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--year', type=int, default=2018, help="Year to evaluate")
    parser.add_argument('--model', type=str, default=None, help="Specific path to model checkpoint")
    parser.add_argument('--sample', type=int, default=1000, help="Number of queries to sample")
    args = parser.parse_args()
    
    # è‡ªåŠ¨æŸ¥æ‰¾æ¨¡å‹ (ä»…å½“æœªæŒ‡å®šæ—¶)
    if args.model is None:
        print("âš ï¸ No model path provided, trying to auto-find latest model...")
        p = Path(Config.OUTPUT_DIR) / f"lgb_batch_end_{args.year}.txt"
        if not p.exists():
             models = list(Path(Config.OUTPUT_DIR).glob("lgb_batch_end_*.txt"))
             if models:
                 p = max(models, key=lambda f: f.stat().st_mtime)
        args.model = str(p)

    run_main(args.year, args.model, args.sample)