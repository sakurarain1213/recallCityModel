"""
ç¦»çº¿æ•°æ®ç”Ÿæˆè„šæœ¬ (ETLä¸€ä½“åŒ–ç‰ˆ)
åŠŸèƒ½: è¯»å–åŸå§‹æ•°æ® -> ç‰¹å¾å·¥ç¨‹ -> æ·±åº¦æ¸…æ´—/å‹ç¼© -> ä¿å­˜ Parquet
ä¼˜åŠ¿: ç›´æ¥ç”Ÿæˆå®šå‹çš„ float32/int16 æ•°æ®ï¼Œæ— éœ€äºŒæ¬¡å¤„ç†
"""
import pandas as pd
import numpy as np
import gc
from pathlib import Path
from src.config import Config
from src.city_data import CityDataLoader
from src.data_loader import load_raw_data_fast
from src.feature_pipeline import FeaturePipeline

# å¼•å…¥ fix_data ä¸­çš„æ ¸å¿ƒæ¸…æ´—é€»è¾‘
def optimize_dataframe(df, verbose=True):
    """æ·±åº¦ä¼˜åŒ– DataFrame å†…å­˜å’Œç±»å‹"""
    start_mem = df.memory_usage(deep=True).sum() / 1024**2
    
    # 1. åˆ é™¤çº¯ Object ç±»å‹çš„æ— ç”¨åˆ— (ä¿ç•™å¿…è¦çš„ ID ç”¨äºåç»­å†å²ç‰¹å¾åŒ¹é…ï¼Œå¦‚æœéœ€è¦çš„è¯)
    # æ³¨æ„ï¼šType_ID_orig å’Œ From_City_orig åœ¨ç”Ÿæˆå†å²ç‰¹å¾æ—¶å¯èƒ½éœ€è¦ï¼Œ
    # ä½†å¦‚æœå½“å‰æ˜¯ä» pipeline å‡ºæ¥çš„æœ€ç»ˆç»“æœï¼Œé€šå¸¸å¯ä»¥åªä¿ç•™æ•°å€¼ IDã€‚
    # è¿™é‡Œæˆ‘ä»¬ä¿ç•™ _orig åç¼€çš„åˆ—ä»¥é˜²ä¸‡ä¸€ï¼Œä½† Hash åŒ– Type_ID
    
    # 1.1 Type_ID å­—ç¬¦ä¸²è½¬ Hash æ•°å€¼
    if 'Type_ID' in df.columns and df['Type_ID'].dtype == 'object':
        df['Type_Hash'] = pd.util.hash_pandas_object(df['Type_ID'], index=False).astype('int64')
        # å¦‚æœæœ‰ Type_ID_orig åˆ™ä¸éœ€è¦ Type_ID äº†
        if 'Type_ID_orig' not in df.columns:
            df['Type_ID_orig'] = df['Type_ID'] # å¤‡ä»½ç”¨äºè·¨å¹´åŒ¹é…
        df = df.drop(columns=['Type_ID'])

    # 1.2 åŸå¸‚ ID æ ‡å‡†åŒ– (æå–æ•°å­—å¹¶è½¬ int16)
    for col in ['From_City', 'To_City']:
        if col in df.columns:
            # å¦‚æœæ˜¯ "åŒ—äº¬(1100)" æ ¼å¼ï¼Œæå–æ•°å­—
            if df[col].dtype == 'object':
                df[col] = df[col].astype(str).str.extract(r'(\d+)', expand=False)
                df[col] = pd.to_numeric(df[col], errors='coerce')
            df[col] = df[col].fillna(0).astype('int16')

    # 2. æ•°å€¼ç±»å‹é™çº§ (æ ¸å¿ƒçœå†…å­˜æ­¥éª¤)
    for col in df.columns:
        # è·³è¿‡å­—ç¬¦ä¸²å¤‡ä»½åˆ—
        if df[col].dtype == 'object': 
            continue
            
        # Float64 -> Float32
        if df[col].dtype == 'float64':
            df[col] = df[col].astype('float32')
        
        # Int64 -> Int32/Int16
        elif df[col].dtype == 'int64':
            c_min, c_max = df[col].min(), df[col].max()
            if c_min >= -32768 and c_max <= 32767:
                df[col] = df[col].astype('int16')
            else:
                df[col] = df[col].astype('int32')

    # 3. åˆ é™¤å®Œå…¨é‡å¤çš„åˆ— (è§£å†³ FeaturePipeline å¯èƒ½äº§ç”Ÿçš„å†—ä½™)
    # è¿™æ˜¯ä¸€ä¸ª O(N^2) æ“ä½œï¼Œä½†åˆ—æ•°ä¸å¤š(å‡ ååˆ—)æ—¶å¾ˆå¿«
    duplicate_cols = []
    cols = df.columns.tolist()
    # åªæ£€æŸ¥æ•°å€¼åˆ—
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    for i in range(len(numeric_cols)):
        col1 = numeric_cols[i]
        for j in range(i + 1, len(numeric_cols)):
            col2 = numeric_cols[j]
            # å¿«é€Ÿæ£€æŸ¥ï¼šå¦‚æœå‡å€¼ä¸åŒè‚¯å®šä¸åŒ
            if df[col1].mean() != df[col2].mean():
                continue
            # è¯¦ç»†æ£€æŸ¥
            if df[col1].equals(df[col2]):
                duplicate_cols.append(col2)
    
    if duplicate_cols:
        df = df.drop(columns=list(set(duplicate_cols)))
        if verbose:
            print(f"  âœ‚ï¸  Removed duplicate cols: {list(set(duplicate_cols))}")

    # 4. ç¼ºå¤±å€¼å¡«å……
    # æ•°å€¼å¡« 0, å­—ç¬¦ä¸²å¡« MISSING
    for col in df.columns:
        if df[col].isnull().any():
            if pd.api.types.is_number(df[col]):
                df[col] = df[col].fillna(0)
            else:
                df[col] = df[col].fillna("MISSING")

    end_mem = df.memory_usage(deep=True).sum() / 1024**2
    if verbose:
        print(f"  ğŸ“‰ Memory optimized: {start_mem:.1f}MB -> {end_mem:.1f}MB")
    
    return df

def generate_all_data(start_year, end_year):
    print(f"ğŸš€ å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ® (ETLä¼˜åŒ–ç‰ˆ) | {start_year}-{end_year}")
    print(f"ğŸ“‚ ç›®æ ‡ç›®å½•: {Config.PROCESSED_DIR}")

    loader = CityDataLoader(Config.DATA_DIR).load_all()
    pipeline = FeaturePipeline(loader, data_dir=Config.PROCESSED_DIR)
    hard_candidates = loader.get_city_ids()

    for year in range(start_year, end_year + 1):
        out_file = Path(Config.PROCESSED_DIR) / f"train_{year}.parquet"
        if out_file.exists():
            print(f"âœ… Year {year} å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            continue

        print(f"\nğŸ“… Processing Year {year}...")

        # 1. åŠ è½½å¹¶é‡‡æ ·
        df = load_raw_data_fast(
            Config.DB_PATH, year, hard_candidates, Config.TOTAL_SAMPLES_PER_QUERY
        )
        if df.empty:
            print(f"âš ï¸  Year {year} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            continue
        print(f"  âœ… Raw loaded: {len(df):,} rows")

        # 2. ç‰¹å¾å·¥ç¨‹
        df = pipeline.transform(df, year, mode='eval', verbose=False)
        
        # 3. æ·±åº¦æ¸…æ´—ä¸å‹ç¼© (æ–°å¢æ­¥éª¤ï¼Œæ›¿ä»£ fix_data.py)
        print("  ğŸ”„ Optimizing data structure...")
        df = optimize_dataframe(df)

        # 4. ä¿å­˜
        df.to_parquet(out_file, index=False, compression='zstd')
        print(f"ğŸ’¾ Saved {out_file.name}")

        del df
        gc.collect()

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆ!")

if __name__ == "__main__":
    # ç”Ÿæˆ 2000-2020 çš„æ•°æ®
    generate_all_data(2000, 2020)